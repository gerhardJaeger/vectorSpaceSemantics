{"cells":[{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Vector space semantics\n\n## Session 02: Count-based models; dimensionality reduction\n\n### Gerhard Jäger\n\n\nMay 2, 2022\n\n(based on slides by Katrin Erk, https://www.katrinerk.com/courses/lin350-computational-semantics, with kind permission)"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Making a count-based distributional model\n\nYou need to make your own count-based distributional model if:\n* you want to study word contexts in a particular genre or text collection\n* you want to have an interpretable model where you can understand what the individual dimensions are\n* you want to compute a distributional model from an amount of data that is comparable to the amount of text a person reads (whoever reads the whole Wikipedia?)\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## A tiny space\n\nWe start with a tiny corpus, so that we can easily inspect our data: "},{"metadata":{"slideshow":{"slide_type":""},"trusted":true},"cell_type":"code","source":"sam_corpus = \"\"\"I am Sam.\nSam I am.\nI do not like green eggs and ham.\"\"\"\n","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is how to split the corpus into sentences using the Natural Language Toolkit:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')","execution_count":2,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","name":"stderr"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsam_sentences = nltk.sent_tokenize(sam_corpus)\nsam_sentences","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"['I am Sam.', 'Sam I am.', 'I do not like green eggs and ham.']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We need to get the context around a target word. To do this, we make a function that, for a target index, yields the words preceding and succeeding (if any). For simplicity, we use a one-word window on either side of the target. \n\n(It has \"yield\" instead of \"return\", so it is intended to be used in a for-loop.)"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"def each_contextword_1wordwindow(wordlist, targetindex):\n    if targetindex > 0:\n        # preceding word\n        yield wordlist[targetindex - 1]\n        \n    if targetindex < len(wordlist)- 1:\n        # succeeding word\n        yield wordlist[targetindex + 1]\n        ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wl = ['a', 'b', 'c', 'd', 'e']\n\nfor x in each_contextword_1wordwindow(wl, 3):\n    print(x)\n","execution_count":5,"outputs":[{"output_type":"stream","text":"c\ne\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Here is how to use it: We count, for each target word, how often each context word appears with it. As an aside, first a few quick words about data structures in NLTK that support us in counting words (or word groups, or pieces of syntactic structure). The first is basically a dictionary mapping words to counts, called a FreqDist. Conveniently, you can just initialize it by giving it a list of items, and it will count how often each item appears in the list:"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# NLTK data structures for counting stuff:\n# count individual words or other items:\n\nfd = nltk.FreqDist([\"a\", \"b\", \"c\", \"a\", \"b\", \"d\"])\nfd","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"FreqDist({'a': 2, 'b': 2, 'c': 1, 'd': 1})"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The second data structure relevant for us today is the ConditionalFreqDist. It also has counts, but it can be used to count, for each target, how often each context word appears, or more generally, how often each word appears given some other word. Say \"a\" is a target, and \"b\" and \"c\" are context items, then a ConditionalFreqDist can be used like a two-deep dictionary, whose first-level keys are called \"conditions\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for targets, count context words,\n# or in general, for one sort of items, \n# count another sort of items\ncfd = nltk.ConditionalFreqDist()\ncfd[\"a\"][\"b\"] += 1\ncfd[\"a\"][\"c\"] += 1\ncfd","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"<ConditionalFreqDist with 1 conditions>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"For the \"condition\" 'a', the entry is again a FreqDist object that counts appearances of 'b' and 'c':"},{"metadata":{"trusted":true},"cell_type":"code","source":"cfd[\"a\"]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"FreqDist({'b': 1, 'c': 1})"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"You can also initialize a ConditionalFreqDist by a list of pairs. It then counts, for each first item of the pair, how often each second item appears. In the next example, the ConditionalFreqDist will record that given \"a\", both \"b\" and \"c\" appeared once, and that given \"d\", \"e\" appeared once:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cfd = nltk.ConditionalFreqDist([(\"a\", \"b\"), (\"a\", \"c\"), (\"d\", \"e\")])\ncfd","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<ConditionalFreqDist with 2 conditions>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfd[\"a\"]","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"FreqDist({'b': 1, 'c': 1})"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now back to our Sam corpus. We can count context words for each target using a ConditionalFreqDist where the conditions are targets, and the keys of the FreqDist's are context words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we will store the target words and their context word counts\n# this is a data type with method  conditions() to get the list of target words,\n# and sam_context_counts[t][cx] gets you the count for target t and context word cx\nsam_context_counts = nltk.ConditionalFreqDist()\n\n# iterate\nfor sentence in sam_sentences:\n    wordlist = nltk.word_tokenize(sentence)\n    \n    for targetindex, target in enumerate(wordlist):\n        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n            sam_context_counts[target][contextword] += 1\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here are the target words from our corpus\nsam_context_counts.conditions()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"['I', 'am', 'Sam', '.', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# Here are all the target/context counts we got from our corpus\nlist(sam_context_counts.items())","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"[('I', FreqDist({'am': 2, 'Sam': 1, 'do': 1})),\n ('am', FreqDist({'I': 2, 'Sam': 1, '.': 1})),\n ('Sam', FreqDist({'am': 1, '.': 1, 'I': 1})),\n ('.', FreqDist({'Sam': 1, 'am': 1, 'ham': 1})),\n ('do', FreqDist({'I': 1, 'not': 1})),\n ('not', FreqDist({'do': 1, 'like': 1})),\n ('like', FreqDist({'not': 1, 'green': 1})),\n ('green', FreqDist({'like': 1, 'eggs': 1})),\n ('eggs', FreqDist({'green': 1, 'and': 1})),\n ('and', FreqDist({'eggs': 1, 'ham': 1})),\n ('ham', FreqDist({'and': 1, '.': 1}))]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here are the counts for one target\nsam_context_counts[\"eggs\"]","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"FreqDist({'green': 1, 'and': 1})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here's an example of how to access one count \n# for target \"I\" and context \"do\"\nsam_context_counts[\"I\"][\"do\"]","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We put our rows and columns in order.\n# For now, our rows are the same as our columns:\n# All target words are also context words, and vice versa.\n# But that doesn't have to be the case.\ntargetlist = sorted(sam_context_counts.conditions())\ncontextlist = sorted(list(set(c for t in sam_context_counts.conditions() \n                              for c in sam_context_counts[t].keys())))\n\nprint(\"Targets\", targetlist)\nprint(\"Contexts\", contextlist)","execution_count":16,"outputs":[{"output_type":"stream","text":"Targets ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\nContexts ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# we can also put all our counts in a pandas dataframe\n\nimport pandas as pd\n\n# we turn our conditional frequency counts into a matrix,\n# filling the empty places (NaN) with zeros \nrows = [sam_context_counts[t] for t in targetlist]\nsam_count_matrix = pd.DataFrame(rows).fillna(0)\n# add row labels as column \"target\", and make it the index\nsam_count_matrix[\"target\"] = targetlist\nsam_count_matrix.set_index(\"target\", inplace = True)\n# reorder all other columns in the order of the context list\nsam_count_matrix = sam_count_matrix.reindex(columns = contextlist, copy = False)\nsam_count_matrix","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"          .    I  Sam   am  and   do  eggs  green  ham  like  not\ntarget                                                           \n.       0.0  0.0  1.0  1.0  0.0  0.0   0.0    0.0  1.0   0.0  0.0\nI       0.0  0.0  1.0  2.0  0.0  1.0   0.0    0.0  0.0   0.0  0.0\nSam     1.0  1.0  0.0  1.0  0.0  0.0   0.0    0.0  0.0   0.0  0.0\nam      1.0  2.0  1.0  0.0  0.0  0.0   0.0    0.0  0.0   0.0  0.0\nand     0.0  0.0  0.0  0.0  0.0  0.0   1.0    0.0  1.0   0.0  0.0\ndo      0.0  1.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0   0.0  1.0\neggs    0.0  0.0  0.0  0.0  1.0  0.0   0.0    1.0  0.0   0.0  0.0\ngreen   0.0  0.0  0.0  0.0  0.0  0.0   1.0    0.0  0.0   1.0  0.0\nham     1.0  0.0  0.0  0.0  1.0  0.0   0.0    0.0  0.0   0.0  0.0\nlike    0.0  0.0  0.0  0.0  0.0  0.0   0.0    1.0  0.0   0.0  1.0\nnot     0.0  0.0  0.0  0.0  0.0  1.0   0.0    0.0  0.0   1.0  0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>.</th>\n      <th>I</th>\n      <th>Sam</th>\n      <th>am</th>\n      <th>and</th>\n      <th>do</th>\n      <th>eggs</th>\n      <th>green</th>\n      <th>ham</th>\n      <th>like</th>\n      <th>not</th>\n    </tr>\n    <tr>\n      <th>target</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>.</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>I</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Sam</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>am</th>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>do</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>eggs</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>green</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ham</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>not</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"Here is a row from our matrix, the vector for \"Sam\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"sam_count_matrix.loc[\"Sam\"]","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":".        1.0\nI        1.0\nSam      0.0\nam       1.0\nand      0.0\ndo       0.0\neggs     0.0\ngreen    0.0\nham      0.0\nlike     0.0\nnot      0.0\nName: Sam, dtype: float64"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Vector distances\n\n- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n- **Euclidean** distance $d_2(\\mathbf u,\\mathbf v)$\n    \n<img src=\"_img/distances01.svg\" width=\"600\">\n\n$$\n    d_2(\\mathbf u,\\mathbf v) = \\sqrt{\n        (u_1-v_1)^2 + \\cdots + (u_n-v_n)^2\n    }\n$$"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Vector distances\n\n- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n- **Manhattan** (“city block”) distance $d_1(\\mathbf u,\\mathbf v)$    \n\n    \n<img src=\"_img/distances02.svg\" width=\"600\">\n\n$$\n    d_1(\\mathbf u,\\mathbf v) = \n        |u_1-v_1| + \\cdots + |u_n-v_n|\n$$\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Vector distances\n\n- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n- both are extensions of the Minkowski **$p$-distance** $d_p(\\mathbf u, \\mathbf v)$ (for $p> 0$)\n\n    \n<img src=\"_img/distances02.svg\" width=\"600\">\n\n$$\n\\begin{aligned}\n    d_p(\\mathbf u,\\mathbf v) &= \n        (|u_1-v_1|^p + \\cdots + |u_n-v_n|^p)^{\\frac{1}{p}}\\\\\n    d_\\infty(\\mathbf u,\\mathbf v) &= \n        \\max_{i=1^n} |u_i-v_i|       \n\\end{aligned}\n$$\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### “circle” for different values of $p$\n\n<img src=\"_img/2D_unit_balls.svg.png\" width=\"3000\">"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## cosine similarity\n\n- length of a vector in count-based model depends on number of occurrences $\\Rightarrow$ uninteresting for our purposes\n- to counter this effect, people often *normalize* vectors\n- length normalization:\n    - replace $\\mathbf v$ by $\\frac{\\mathbf v}{\\|\\mathbf v\\|}$\n    - $\\|\\mathbf v\\|$ is the “norm” (length) of the vector\n    $$\n    \\|\\mathbf v\\| = \\sqrt{v_1^2 + \\cdots + v_n^2}\n    $$\n- vectors of norm $=1$ are called **unit vectors**    \n- a convenient measure of the similarity between two unit vectors is the cosine of the angle between them\n\n<img src=\"_img/trigonometry.svg\" width=600>\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## cosine similarity\n\n- numerically: \n    - for unit vectors, the cosine of the angle $\\theta$ between vectors $\\mathbf u$ and $\\mathbf v$ equals the *inner product* (also called “dot product”)\n    \n    $$\n    \\cos \\theta = \\langle \\mathbf u, \\mathbf v\\rangle = \\mathbf u \\cdot \\mathbf v = \\sum u_iv_i +\\cdots+u_nv_n\n    $$\n    - for vectors in general\n    $$\\cos\\theta = \\frac{\\mathbf u\\cdot\\mathbf v}{\\|\\mathbf u\\|\\|\\mathbf v\\|}$$"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## cosine similarity\n\nNow we can compute cosine similarity in our space. Python has a built-in function for this – with one catch: It computes 1 - cosine, as a distance. Here's how to get back to cosine similarity:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\n\ndef cosine_sim(vec1, vec2):\n    return 1 - scipy.spatial.distance.cosine(vec1, vec2)\n\ncosine_sim(sam_count_matrix.loc[\"I\"], sam_count_matrix.loc[\"Sam\"])\n\n","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"0.4714045207910318"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"\n\n## From word co-occurrence counts to association weights\n\nAs we discussed in class, raw frequency counts may not be what we want -- we don't need to know that all words co-occur a lot with \"the\" and \"a\". Even if we ditch stopwords, the frequency bias in the data may not be what we want: Do we need to know that all words co-occur a lot with \"said\"? \n\nSeveral methods have been developed for going from counts to association weights, including tf/idf and pointwise mutual information. Here, we demonstrate how to compute pointwise mutual information, defined as\n\n$PMI(a, b) = \\log \\frac{P(a, b)}{P(a)P(b)}$ \n\nIn the numerator, we have the joint probability of a *and* b. The formula compares this to the denominator, which has the product of the probability of a and the probability of b: If a and b were completely independent, had zero association, we would expect them to co-occur only by chance, that is, we would expect $P(a, b) = P(a)P(b)$. If $P(a, b)$ is larger than $P(a)P(b)$, then a and b are positively associated -- they co-occur more often than you would expect just from chance encounters. If $P(a, b)$ is smaller than $P(a)P(b)$, then a and b are negatively associated -- they really don't want to go together. \n\nIn practice, we are often not interested in negative associations, and only use positive ones. Then we get PPMI:\n\n\n$PPMI(a, b) = \\left\\{\\begin{array}{ll}PMI(a, b) & \\text{if } PMI(a, b) > 0\\\\\n0 & \\text{else}\n\\end{array}\\right.$\n"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"\n\n\n# Here are the pieces we need:\n# pointwise mutual information (PMI):\n#                    P(t, c)\n# PMI(t, c) = log --------------\n#                   P(t) P(c)\n#\n#    #(t, c): the co-occurrence count of t with c\n#    #(_, _): the sum of counts in the whole table, across all targets\n#    #(t, _): the sum of counts in the row of target t\n#    #(_, c): the sum of counts in the column of context item c\n#\n# then\n# P(t, c) = #(t, c) / #(_, _)\n# P(t) = #(t, _) / #(_, _)\n# P(c) = #(_, c) / #(_, _)\n#\n# PPMI(t, c) = { PMI(t, c) if PMI(t, c) >= 0\n#                0, else\n\n# target count #(t, _):\nprint(\"target count for Sam\", sam_count_matrix.loc[\"Sam\"].sum())\n\n# overall count #(_, _):\nprint(\"overall count\", sum(sam_count_matrix.sum()))\n\n# context item count #(_, c):\nprint(\"context item count for eggs\", sam_count_matrix[\"eggs\"].sum())","execution_count":20,"outputs":[{"output_type":"stream","text":"target count for Sam 3.0\noverall count 28.0\ncontext item count for eggs 2.0\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"import math\n\n# we'll store the association weights in a dictionary for now\nsam_pmi = { }\n\ncount_all = sum(sam_count_matrix.sum())\n\nfor target in sam_count_matrix.index:\n    for context in sam_count_matrix.columns:\n        p_t_c = sam_count_matrix.loc[target][context] / count_all\n        # print(\"p_t_c\", target, context, p_t_c)\n        p_t = sam_count_matrix.loc[target].sum() / count_all\n        # print(\"p_t\", target, p_t)\n        p_c = sam_count_matrix[context].sum() / count_all\n        # print(\"p_c\", context, p_c)\n\n        # we need to watch out: if p_t_c is zero, the logarithm is undefined\n        if p_t_c == 0.0 or p_t == 0.0 or p_c == 0.0:\n            pmi = 0.0\n        else: \n            pmi = math.log( p_t_c / (p_t * p_c))\n        \n        # print(\"pmi\", target, context, pmi)\n        \n        if target not in sam_pmi: sam_pmi[ target] = { }\n        sam_pmi[target][context]= pmi\n\n# And we again store the result in a data frame\nrows = [sam_pmi[t] for t in targetlist]\nsam_pmi_matrix = pd.DataFrame(rows).fillna(0)\n# set the target as the index\nsam_pmi_matrix[\"target\"] = targetlist\nsam_pmi_matrix.set_index(\"target\", inplace = True)\n# and reorder columns to match our canonical column order\nsam_pmi_matrix = sam_pmi_matrix.reindex(columns = contextlist, copy = False)\nsam_pmi_matrix","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"               .         I       Sam        am      and        do     eggs  \\\ntarget                                                                       \n.       0.000000  0.000000  1.134980  0.847298  0.00000  0.000000  0.00000   \nI       0.000000  0.000000  0.847298  1.252763  0.00000  1.252763  0.00000   \nSam     1.134980  0.847298  0.000000  0.847298  0.00000  0.000000  0.00000   \nam      0.847298  1.252763  0.847298  0.000000  0.00000  0.000000  0.00000   \nand     0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  1.94591   \ndo      0.000000  1.252763  0.000000  0.000000  0.00000  0.000000  0.00000   \neggs    0.000000  0.000000  0.000000  0.000000  1.94591  0.000000  0.00000   \ngreen   0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  1.94591   \nham     1.540445  0.000000  0.000000  0.000000  1.94591  0.000000  0.00000   \nlike    0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \nnot     0.000000  0.000000  0.000000  0.000000  0.00000  1.945910  0.00000   \n\n          green       ham     like      not  \ntarget                                       \n.       0.00000  1.540445  0.00000  0.00000  \nI       0.00000  0.000000  0.00000  0.00000  \nSam     0.00000  0.000000  0.00000  0.00000  \nam      0.00000  0.000000  0.00000  0.00000  \nand     0.00000  1.945910  0.00000  0.00000  \ndo      0.00000  0.000000  0.00000  1.94591  \neggs    1.94591  0.000000  0.00000  0.00000  \ngreen   0.00000  0.000000  1.94591  0.00000  \nham     0.00000  0.000000  0.00000  0.00000  \nlike    1.94591  0.000000  0.00000  1.94591  \nnot     0.00000  0.000000  1.94591  0.00000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>.</th>\n      <th>I</th>\n      <th>Sam</th>\n      <th>am</th>\n      <th>and</th>\n      <th>do</th>\n      <th>eggs</th>\n      <th>green</th>\n      <th>ham</th>\n      <th>like</th>\n      <th>not</th>\n    </tr>\n    <tr>\n      <th>target</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>.</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.134980</td>\n      <td>0.847298</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.540445</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>I</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.847298</td>\n      <td>1.252763</td>\n      <td>0.00000</td>\n      <td>1.252763</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>Sam</th>\n      <td>1.134980</td>\n      <td>0.847298</td>\n      <td>0.000000</td>\n      <td>0.847298</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>am</th>\n      <td>0.847298</td>\n      <td>1.252763</td>\n      <td>0.847298</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.00000</td>\n      <td>1.945910</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>do</th>\n      <td>0.000000</td>\n      <td>1.252763</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.94591</td>\n    </tr>\n    <tr>\n      <th>eggs</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.94591</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>green</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>ham</th>\n      <td>1.540445</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.94591</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.94591</td>\n    </tr>\n    <tr>\n      <th>not</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.945910</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# We can directly compare counts and associations\nprint(\"Counts for dot:\")\nprint(\"\\t\", sam_count_matrix.loc[\".\"])\nprint(\"Associations for dot:\")\nprint(\"\\t\", sam_pmi_matrix.loc[\".\"])\n","execution_count":22,"outputs":[{"output_type":"stream","text":"Counts for dot:\n\t .        0.0\nI        0.0\nSam      1.0\nam       1.0\nand      0.0\ndo       0.0\neggs     0.0\ngreen    0.0\nham      1.0\nlike     0.0\nnot      0.0\nName: ., dtype: float64\nAssociations for dot:\n\t .        0.000000\nI        0.000000\nSam      1.134980\nam       0.847298\nand      0.000000\ndo       0.000000\neggs     0.000000\ngreen    0.000000\nham      1.540445\nlike     0.000000\nnot      0.000000\nName: ., dtype: float64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How has that changed cosines?\nprint(\"Cosine of 'I' and 'Sam', count-based\", \n      cosine_sim(sam_count_matrix.loc[\"I\"], sam_count_matrix.loc[\"Sam\"]))\nprint(\"Cosine of 'I' and 'Sam', pmi-based\", \n      cosine_sim(sam_pmi_matrix.loc[\"I\"], sam_pmi_matrix.loc[\"Sam\"]))","execution_count":23,"outputs":[{"output_type":"stream","text":"Cosine of 'I' and 'Sam', count-based 0.4714045207910318\nCosine of 'I' and 'Sam', pmi-based 0.3274843356832646\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# Here is how to get all pairwise cosines in our matrix:\n\n# Step 1: this computes pairwise cosine *distances*, 1 - cosine\nsam_cosine_dist = scipy.spatial.distance.cdist(sam_pmi_matrix, sam_pmi_matrix, metric = \"cosine\")\n\n# Step 2: convert to cosine similarity\nsam_cosine_sim = 1 - sam_cosine_dist\n\n# Let's look at this. sam_cosine_sim is a numpy ndarray, which\n# has a method round() for rounding.\n# As you can see, the cosine similarity of each word with itself is 1,\n# and the value for \"i\" and \"sam\", row 2, column 3, is 0.33, as computed above.\nsam_cosine_sim.round(2)","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"array([[1.  , 0.49, 0.21, 0.27, 0.52, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.49, 1.  , 0.33, 0.21, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.45],\n       [0.21, 0.33, 1.  , 0.71, 0.  , 0.28, 0.  , 0.  , 0.43, 0.  , 0.  ],\n       [0.27, 0.21, 0.71, 1.  , 0.  , 0.39, 0.  , 0.  , 0.3 , 0.  , 0.  ],\n       [0.52, 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.28, 0.39, 0.  , 1.  , 0.  , 0.  , 0.  , 0.59, 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.55, 0.5 , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 1.  , 0.  , 0.  , 0.5 ],\n       [0.  , 0.  , 0.43, 0.3 , 0.  , 0.  , 0.55, 0.  , 1.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.59, 0.5 , 0.  , 0.  , 1.  , 0.  ],\n       [0.  , 0.45, 0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 1.  ]])"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Nearest neighbors\n\nWe want to know about a word's nearest neighbors. Computing this by hand is a major pain: You would have to compute all pairwise cosines, and then rummage through them to find the maximum. In our tiny Sam corpus, this is feasible, but not in a large corpus. Fortunatly scikit-learn has a function NearestNeighbors that can do the work for us. One downside: It does not know cosine similarity outright. \n\nFirst option: We give it the cosine distance function that we used above. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\n# we make a nearest-neighbors object and tell it we'll always want the 3 nearest neighbors at a time\nnearest_neighbors_obj = NearestNeighbors(n_neighbors=3, metric = scipy.spatial.distance.cosine)\n\n# we then allow it to compute an internal datastructure from our data\nnearest_neighbors_obj.fit(sam_pmi_matrix)","execution_count":25,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'sklearn'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NearestNeighbors\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# we make a nearest-neighbors object and tell it we'll always want the 3 nearest neighbors at a time\u001b[39;00m\n\u001b[1;32m      4\u001b[0m nearest_neighbors_obj \u001b[38;5;241m=\u001b[39m NearestNeighbors(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, metric \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39mspatial\u001b[38;5;241m.\u001b[39mdistance\u001b[38;5;241m.\u001b[39mcosine)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([sam_pmi_matrix.loc[\"Sam\"]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cosine_distances and target_indices are both two-dimensional arrays. Let's extract \n# lists of values\ncosine_distances = cosine_distances[0].tolist()\ntarget_indices = target_indices[0].tolist()\n\nfor cosinedist, targetindex in zip(cosine_distances, target_indices):\n    print(\"Neighbor is\", targetlist[targetindex], \"with similarity\", 1 - cosinedist)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"Second option: We use Euclidean distance (walking distance), but first normalize all vectors to be of length one. This won't give us the same distance values, but the same orderings of what is nearest. See https://stackoverflow.com/questions/34144632/using-cosine-distance-with-scikit-learn-kneighborsclassifier\n\nRecall that the length of a vector is defined as \n\n$||a|| = \\sqrt{\\sum_i a_i^2}$\n\nThe Python package numpy has an implementation of that. For a vector `a`, we need to call `numpy.linalg.norm(a, ord = 2)`.\n\nNote: The sum of values of a vector is called its \"L1 norm\", and the vector length is called its \"L2 norm\". That's why we need the parameter `ord = 2`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nrows = [ sam_pmi_matrix.loc[t] / \n        np.linalg.norm(sam_pmi_matrix.loc[t], ord = 2)\n        for t in targetlist ]\n   \nsam_pminorm_matrix = pd.DataFrame(rows)\n\nsam_pminorm_matrix","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# we make a nearest-neighbors object \n# and tell it we'll always want the 2 nearest neighbors at a time.\n# Distance metric is default: \n# minkowski with p=2, which is equivalent to Euclidean distance\nnearest_neighbors_obj_2 = NearestNeighbors(n_neighbors=3)\n\n# we then allow it to compute an internal datastructure from our data\nnearest_neighbors_obj_2.fit(sam_pminorm_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and let's look at nearest neighbors again\ndistances, target_indices = nearest_neighbors_obj_2.kneighbors([sam_pmi_matrix.loc[\"Sam\"]])\n\n# cosine_distances and target_indices are both two-dimensional arrays. Let's extract \n# lists of values\ndistances = distances[0].tolist()\ntarget_indices = target_indices[0].tolist()\n\nfor dist, targetindex in zip(distances, target_indices):\n    print(\"Neighbor is\", targetlist[targetindex], \"with distance\", dist)","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Using a somewhat larger corpus\n\nWe next demonstrate a somewhat larger corpus, with yet another method of accessing the corpus data: If the data is available within the NLTK corpora, you can use the NLTK's corpus reader to access it.\n\nThe Brown corpus is a 1 million word corpus of carefully selected text pieces from different genres, originally made to support dictionary-makers, so it's intended to cover a broad variety of genres in English."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The first few sentences of the Brown corpus:\\n\")\nfor s in nltk.corpus.brown.sents()[:3]: \n    print(s, \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"We compute the target/context counts, noting context items as we go. We only count words that appear at least 10 times in the corpus. This cuts down a lot on the size of our matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"brown_wordcounts = nltk.FreqDist(nltk.corpus.brown.words())\nbrown_wordcounts.most_common(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\nbrown_context_counts = nltk.ConditionalFreqDist()\n\nfrequency_threshold = 20\n\nfor sentence in nltk.corpus.brown.sents():\n    # remove punctuation.\n    # at this point you could also remove stopwords\n    # or iterate over sents_tagged() instead of sents()\n    # to get parts of speech, and only retain\n    # content words\n    wordlist = [w for w in sentence if w.strip(string.punctuation) != \"\"]\n    \n    for targetindex, target in enumerate(wordlist):\n        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n                brown_context_counts[target][contextword] += 1   \n","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"For this larger corpus, it now makes sense to look at some context word counts to get a sense of what the tables of counts tell us. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 most frequent context words: similar across many items\n# (what can we do about that?)\nprint(\"10 most frequent contexts for some targets:\\n\")\nprint(\"election:\\n\", brown_context_counts[\"election\"].most_common(10))\nprint(\"love:\\n\", brown_context_counts[\"love\"].most_common(10))\nprint(\"car:\", brown_context_counts[\"car\"].most_common(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 100 most frequent context words: now we are starting to see differences.\n# We also see that many of the 100 most frequent context words only have counts of one.\nprint(\"100 most frequent contexts for some targets:\\n\")\nprint(\"election:\\n\", brown_context_counts[\"election\"].most_common(100))\nprint(\"love:\\n\", brown_context_counts[\"love\"].most_common(100))\nprint(\"car:\\n\", brown_context_counts[\"car\"].most_common(100))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# some ambiguous words\nprint(\"Some ambiguous words:\")\nprint(\"bat:\\n\", brown_context_counts[\"bat\"].most_common(100))\nprint(\"bank:\\n\", brown_context_counts[\"bank\"].most_common(100))\nprint(\"bar:\\n\", brown_context_counts[\"bar\"].most_common(100))\nprint(\"leave:\\n\", brown_context_counts[\"leave\"].most_common(100))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Working with numpy matrices\n\nWe re-compute the whole space as a matrix, this time using numpy, because this corpus is already too big to fit comfortably into pandas. (Meaning, it takes forever to compute the dataframes.) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# first determine the list of all words in Brown\n# repeat: frequency threshold\nfrequency_threshold = 20\n\nbrown_wordlist = list(w for w in brown_wordcounts if brown_wordcounts[w] >= frequency_threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a dictionary that maps each word to its index in the wordlist\nbrown_wordlist_lookup = dict((word, index) for index, word in enumerate(brown_wordlist))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# We need an array with enough space to hold \n# len(brown_wordlist) target words, and\n# len(brown_wordlist) context words.\n# We first initialize it to all zeros.\nbrown_count_matrix = np.zeros((len(brown_wordlist), len(brown_wordlist)))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# Now, let's do the context word counting with this matrix.\n\nimport string\n\nfor sentence in nltk.corpus.brown.sents():\n    # remove punctuation.\n    # at this point you could also remove stopwords\n    # or iterate over sents_tagged() instead of sents()\n    # to get parts of speech, and only retain\n    # content words\n    wordlist = [w for w in sentence if w.strip(string.punctuation) != \"\"]\n    \n    for targetindex, target in enumerate(wordlist):\n        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n                # which cell in the matrix is this? \n                # look up both the target and the context word\n                # in the ordered list of Brown words\n                targetindex_matrix = brown_wordlist_lookup[target]\n                contextindex_matrix = brown_wordlist_lookup[contextword]\n                # and add a count of one for this cell in the matrix\n                brown_count_matrix[targetindex_matrix][contextindex_matrix] += 1   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can again compute similarity in this space\nsaid_index = brown_wordlist_lookup[\"said\"]\nwrote_index = brown_wordlist_lookup[\"wrote\"]\ncosine_sim( brown_count_matrix[said_index], brown_count_matrix[wrote_index])","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"We now compute PMI again, making use of the fact that\n\n$PMI(a, b) = \\log \\frac{P(a, b)}{P(a)P(b)} = \\log\\frac{P(b|a)}{P(b)}$\n\nnumpy offers functions that apply an operation to a whole vector at once,\nrather than one at a time. \nThis is much quicker."},{"metadata":{"trusted":true},"cell_type":"code","source":"# And we can compute PMI again.\n# numpy offers functions that\n# apply an operation to a whole vector at once,\n# rather than one at a time. \n# This is much quicker.\n\ncount_all = brown_count_matrix.sum()\n\n# probability of contexts/columns:\n# this is our P(b)\n# This is a vector with a probability for each context\n# sum(axis=0) is numpy's way of saying that we want to sum each column\ncol_totals = brown_count_matrix.sum(axis=0)\n# avoid zeros, they get us in trouble later when we divide by p_c\ncol_totals[col_totals == 0] = 0.00001\n# this is a vector where each row total is divided by the overall count\np_c = col_totals / count_all\n\n# probability of context given target:\n# this is our P(b|a)\n# sum(axis=1) is numpy's way of saying that we want to sum each row:\n# we divide each row by its row total, getting the probability of a context item\n# within this target. \n# do do this, we flip the matrix on its side so that columns become rows,\n# then do the division (otherwise numpy would do column-wise instead of row-wise division)\nrow_totals = brown_count_matrix.sum(axis=1).astype(float)\nrow_totals[row_totals == 0] = 0.00001\np_c_given_t = (brown_count_matrix.T / row_totals).T\n\n# PMI: log( P(b|a) / P(b))\n# we again divide a matrix by a vector\n# this time we do want column-wise division\n# so we don't have to flip the matrix\npct_divided_by_pc = p_c_given_t / p_c\n# avoid doing log(0)by replacing 0 by a small number\npct_divided_by_pc[pct_divided_by_pc==0] = 0.00001\n\nbrown_pmi_matrix = np.log(pct_divided_by_pc)\n","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# and computing similarity again\nsaid_index = brown_wordlist_lookup[\"said\"]\nwrote_index = brown_wordlist_lookup[\"wrote\"]\ncosine_sim( brown_pmi_matrix[said_index], brown_pmi_matrix[wrote_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nearest_neighbors_obj = NearestNeighbors(n_neighbors=20, metric = scipy.spatial.distance.cosine)\n\n# we then allow it to compute an internal datastructure from our data\nnearest_neighbors_obj.fit(brown_pmi_matrix)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([brown_pmi_matrix[said_index]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saidNN = pd.DataFrame(\n    np.c_[\n        np.array([brown_wordlist[i] for i in [target_indices][0][0]]), \n        cosine_distances[0]\n    ],\n    columns = ['word', 'cosine similarity']\n)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saidNN","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Creating a term-document matrix\n\nData are taken from https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"import os\ntry:\n    os.makedirs('../data')\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://unitc-my.sharepoint.com/:x:/g/personal/nwsja01_cloud_uni-tuebingen_de/ESJ8QyqzmxpInK_jMQvQ_LEB9XoRbcw39rFmM6ZAQjrwvA?download=1 -O ../data/Reviews.csv","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../data/Reviews.csv\")\ndf","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"np.shape(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.10.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":2}