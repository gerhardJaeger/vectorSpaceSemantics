{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector space semantics\n",
    "\n",
    "## Session 02: Count-based models; dimensionality reduction\n",
    "\n",
    "### Gerhard JÃ¤ger\n",
    "\n",
    "\n",
    "May 2, 2022\n",
    "\n",
    "(based on slides by Katrin Erk, https://www.katrinerk.com/courses/lin350-computational-semantics, with kind permission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making a count-based distributional model\n",
    "\n",
    "You need to make your own count-based distributional model if:\n",
    "* you want to study word contexts in a particular genre or text collection\n",
    "* you want to have an interpretable model where you can understand what the individual dimensions are\n",
    "* you want to compute a distributional model from an amount of data that is comparable to the amount of text a person reads (whoever reads the whole Wikipedia?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A tiny space\n",
    "\n",
    "We start with a tiny corpus, so that we can easily inspect our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "sam_corpus = \"\"\"I am Sam.\n",
    "Sam I am.\n",
    "I do not like green eggs and ham.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to split the corpus into sentences using the Natural Language Toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Sam.', 'Sam I am.', 'I do not like green eggs and ham.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sam_sentences = nltk.sent_tokenize(sam_corpus)\n",
    "sam_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the context around a target word. To do this, we make a function that, for a target index, yields the words preceding and succeeding (if any). For simplicity, we use a one-word window on either side of the target. \n",
    "\n",
    "(It has \"yield\" instead of \"return\", so it is intended to be used in a for-loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def each_contextword_1wordwindow(wordlist, targetindex):\n",
    "    if targetindex > 0:\n",
    "        # preceding word\n",
    "        yield wordlist[targetindex - 1]\n",
    "        \n",
    "    if targetindex < len(wordlist)- 1:\n",
    "        # succeeding word\n",
    "        yield wordlist[targetindex + 1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "wl = ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "for x in each_contextword_1wordwindow(wl, 3):\n",
    "    print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to use it: We count, for each target word, how often each context word appears with it. As an aside, first a few quick words about data structures in NLTK that support us in counting words (or word groups, or pieces of syntactic structure). The first is basically a dictionary mapping words to counts, called a FreqDist. Conveniently, you can just initialize it by giving it a list of items, and it will count how often each item appears in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 2, 'b': 2, 'c': 1, 'd': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK data structures for counting stuff:\n",
    "# count individual words or other items:\n",
    "\n",
    "fd = nltk.FreqDist([\"a\", \"b\", \"c\", \"a\", \"b\", \"d\"])\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second data structure relevant for us today is the ConditionalFreqDist. It also has counts, but it can be used to count, for each target, how often each context word appears, or more generally, how often each word appears given some other word. Say \"a\" is a target, and \"b\" and \"c\" are context items, then a ConditionalFreqDist can be used like a two-deep dictionary, whose first-level keys are called \"conditions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 1 conditions>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for targets, count context words,\n",
    "# or in general, for one sort of items, \n",
    "# count another sort of items\n",
    "cfd = nltk.ConditionalFreqDist()\n",
    "cfd[\"a\"][\"b\"] += 1\n",
    "cfd[\"a\"][\"c\"] += 1\n",
    "cfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"condition\" 'a', the entry is again a FreqDist object that counts appearances of 'b' and 'c':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 1, 'c': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also initialize a ConditionalFreqDist by a list of pairs. It then counts, for each first item of the pair, how often each second item appears. In the next example, the ConditionalFreqDist will record that given \"a\", both \"b\" and \"c\" appeared once, and that given \"d\", \"e\" appeared once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 2 conditions>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist([(\"a\", \"b\"), (\"a\", \"c\"), (\"d\", \"e\")])\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 1, 'c': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now back to our Sam corpus. We can count context words for each target using a ConditionalFreqDist where the conditions are targets, and the keys of the FreqDist's are context words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will store the target words and their context word counts\n",
    "# this is a data type with method  conditions() to get the list of target words,\n",
    "# and sam_context_counts[t][cx] gets you the count for target t and context word cx\n",
    "sam_context_counts = nltk.ConditionalFreqDist()\n",
    "\n",
    "# iterate\n",
    "for sentence in sam_sentences:\n",
    "    wordlist = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    for targetindex, target in enumerate(wordlist):\n",
    "        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n",
    "            sam_context_counts[target][contextword] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'Sam', '.', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the target words from our corpus\n",
    "sam_context_counts.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', FreqDist({'am': 2, 'Sam': 1, 'do': 1})),\n",
       " ('am', FreqDist({'I': 2, 'Sam': 1, '.': 1})),\n",
       " ('Sam', FreqDist({'am': 1, '.': 1, 'I': 1})),\n",
       " ('.', FreqDist({'Sam': 1, 'am': 1, 'ham': 1})),\n",
       " ('do', FreqDist({'I': 1, 'not': 1})),\n",
       " ('not', FreqDist({'do': 1, 'like': 1})),\n",
       " ('like', FreqDist({'not': 1, 'green': 1})),\n",
       " ('green', FreqDist({'like': 1, 'eggs': 1})),\n",
       " ('eggs', FreqDist({'green': 1, 'and': 1})),\n",
       " ('and', FreqDist({'eggs': 1, 'ham': 1})),\n",
       " ('ham', FreqDist({'and': 1, '.': 1}))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are all the target/context counts we got from our corpus\n",
    "list(sam_context_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'green': 1, 'and': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the counts for one target\n",
    "sam_context_counts[\"eggs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of how to access one count \n",
    "# for target \"I\" and context \"do\"\n",
    "sam_context_counts[\"I\"][\"do\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\n",
      "Contexts ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\n"
     ]
    }
   ],
   "source": [
    "# We put our rows and columns in order.\n",
    "# For now, our rows are the same as our columns:\n",
    "# All target words are also context words, and vice versa.\n",
    "# But that doesn't have to be the case.\n",
    "targetlist = sorted(sam_context_counts.conditions())\n",
    "contextlist = sorted(list(set(c for t in sam_context_counts.conditions() \n",
    "                              for c in sam_context_counts[t].keys())))\n",
    "\n",
    "print(\"Targets\", targetlist)\n",
    "print(\"Contexts\", contextlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>I</th>\n",
       "      <th>Sam</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>do</th>\n",
       "      <th>eggs</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>like</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          .    I  Sam   am  and   do  eggs  green  ham  like  not\n",
       "target                                                           \n",
       ".       0.0  0.0  1.0  1.0  0.0  0.0   0.0    0.0  1.0   0.0  0.0\n",
       "I       0.0  0.0  1.0  2.0  0.0  1.0   0.0    0.0  0.0   0.0  0.0\n",
       "Sam     1.0  1.0  0.0  1.0  0.0  0.0   0.0    0.0  0.0   0.0  0.0\n",
       "am      1.0  2.0  1.0  0.0  0.0  0.0   0.0    0.0  0.0   0.0  0.0\n",
       "and     0.0  0.0  0.0  0.0  0.0  0.0   1.0    0.0  1.0   0.0  0.0\n",
       "do      0.0  1.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0   0.0  1.0\n",
       "eggs    0.0  0.0  0.0  0.0  1.0  0.0   0.0    1.0  0.0   0.0  0.0\n",
       "green   0.0  0.0  0.0  0.0  0.0  0.0   1.0    0.0  0.0   1.0  0.0\n",
       "ham     1.0  0.0  0.0  0.0  1.0  0.0   0.0    0.0  0.0   0.0  0.0\n",
       "like    0.0  0.0  0.0  0.0  0.0  0.0   0.0    1.0  0.0   0.0  1.0\n",
       "not     0.0  0.0  0.0  0.0  0.0  1.0   0.0    0.0  0.0   1.0  0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also put all our counts in a pandas dataframe\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# we turn our conditional frequency counts into a matrix,\n",
    "# filling the empty places (NaN) with zeros \n",
    "rows = [sam_context_counts[t] for t in targetlist]\n",
    "sam_count_matrix = pd.DataFrame(rows).fillna(0)\n",
    "# add row labels as column \"target\", and make it the index\n",
    "sam_count_matrix[\"target\"] = targetlist\n",
    "sam_count_matrix.set_index(\"target\", inplace = True)\n",
    "# reorder all other columns in the order of the context list\n",
    "sam_count_matrix = sam_count_matrix.reindex(columns = contextlist, copy = False)\n",
    "sam_count_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is a row from our matrix, the vector for \"Sam\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".        1.0\n",
       "I        1.0\n",
       "Sam      0.0\n",
       "am       1.0\n",
       "and      0.0\n",
       "do       0.0\n",
       "eggs     0.0\n",
       "green    0.0\n",
       "ham      0.0\n",
       "like     0.0\n",
       "not      0.0\n",
       "Name: Sam, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_count_matrix.loc[\"Sam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector distances\n",
    "\n",
    "- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n",
    "    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n",
    "    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n",
    "- **Euclidean** distance $d_2(\\mathbf u,\\mathbf v)$\n",
    "    \n",
    "<img src=\"_img/distances01.svg\" width=\"600\">\n",
    "\n",
    "$$\n",
    "    d_2(\\mathbf u,\\mathbf v) = \\sqrt{\n",
    "        (u_1-v_1)^2 + \\cdots + (u_n-v_n)^2\n",
    "    }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector distances\n",
    "\n",
    "- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n",
    "    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n",
    "    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n",
    "- **Manhattan** (âcity blockâ) distance $d_1(\\mathbf u,\\mathbf v)$    \n",
    "\n",
    "    \n",
    "<img src=\"_img/distances02.svg\" width=\"600\">\n",
    "\n",
    "$$\n",
    "    d_1(\\mathbf u,\\mathbf v) = \n",
    "        |u_1-v_1| + \\cdots + |u_n-v_n|\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector distances\n",
    "\n",
    "- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n",
    "    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n",
    "    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n",
    "- both are extensions of the Minkowski **$p$-distance** $d_p(\\mathbf u, \\mathbf v)$ (for $p> 0$)\n",
    "\n",
    "    \n",
    "<img src=\"_img/distances02.svg\" width=\"600\">\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    d_p(\\mathbf u,\\mathbf v) &= \n",
    "        (|u_1-v_1|^p + \\cdots + |u_n-v_n|^p)^{\\frac{1}{p}}\\\\\n",
    "    d_\\infty(\\mathbf u,\\mathbf v) &= \n",
    "        \\max_{i=1^n} |u_i-v_i|       \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### âcircleâ for different values of $p$\n",
    "\n",
    "<img src=\"_img/2D_unit_balls.svg.png\" width=\"3000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## cosine similarity\n",
    "\n",
    "- length of a vector in count-based model depends on number of occurrences $\\Rightarrow$ uninteresting for our purposes\n",
    "- to counter this effect, people often *normalize* vectors\n",
    "- length normalization:\n",
    "    - replace $\\mathbf v$ by $\\frac{\\mathbf v}{\\|\\mathbf v\\|}$\n",
    "    - $\\|\\mathbf v\\|$ is the ânormâ (length) of the vector\n",
    "    $$\n",
    "    \\|\\mathbf v\\| = \\sqrt{v_1^2 + \\cdots + v_n^2}\n",
    "    $$\n",
    "- vectors of norm $=1$ are called **unit vectors**    \n",
    "- a convenient measure of the similarity between two unit vectors is the cosine of the angle between them\n",
    "\n",
    "<img src=\"_img/trigonometry.svg\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## cosine similarity\n",
    "\n",
    "- numerically: \n",
    "    - for unit vectors, the cosine of the angle $\\theta$ between vectors $\\mathbf u$ and $\\mathbf v$ equals the *inner product* (also called âdot productâ)\n",
    "    \n",
    "    $$\n",
    "    \\cos \\theta = \\langle \\mathbf u, \\mathbf v\\rangle = \\mathbf u \\cdot \\mathbf v = \\sum u_iv_i +\\cdots+u_nv_n\n",
    "    $$\n",
    "    - for vectors in general\n",
    "    $$\\cos\\theta = \\frac{\\mathbf u\\cdot\\mathbf v}{\\|\\mathbf u\\|\\|\\mathbf v\\|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## cosine similarity\n",
    "\n",
    "Now we can compute cosine similarity in our space. Python has a built-in function for this â with one catch: It computes 1 - cosine, as a distance. Here's how to get back to cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714045207910318"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    return 1 - scipy.spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "cosine_sim(sam_count_matrix.loc[\"I\"], sam_count_matrix.loc[\"Sam\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## From word co-occurrence counts to association weights\n",
    "\n",
    "As we discussed in class, raw frequency counts may not be what we want -- we don't need to know that all words co-occur a lot with \"the\" and \"a\". Even if we ditch stopwords, the frequency bias in the data may not be what we want: Do we need to know that all words co-occur a lot with \"said\"? \n",
    "\n",
    "Several methods have been developed for going from counts to association weights, including tf/idf and pointwise mutual information. Here, we demonstrate how to compute pointwise mutual information, defined as\n",
    "\n",
    "$PMI(a, b) = \\log \\frac{P(a, b)}{P(a)P(b)}$ \n",
    "\n",
    "In the numerator, we have the joint probability of a *and* b. The formula compares this to the denominator, which has the product of the probability of a and the probability of b: If a and b were completely independent, had zero association, we would expect them to co-occur only by chance, that is, we would expect $P(a, b) = P(a)P(b)$. If $P(a, b)$ is larger than $P(a)P(b)$, then a and b are positively associated -- they co-occur more often than you would expect just from chance encounters. If $P(a, b)$ is smaller than $P(a)P(b)$, then a and b are negatively associated -- they really don't want to go together. \n",
    "\n",
    "In practice, we are often not interested in negative associations, and only use positive ones. Then we get PPMI:\n",
    "\n",
    "\n",
    "$PPMI(a, b) = \\left\\{\\begin{array}{ll}PMI(a, b) & \\text{if } PMI(a, b) > 0\\\\\n",
    "0 & \\text{else}\n",
    "\\end{array}\\right.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target count for Sam 3.0\n",
      "overall count 28.0\n",
      "context item count for eggs 2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Here are the pieces we need:\n",
    "# pointwise mutual information (PMI):\n",
    "#                    P(t, c)\n",
    "# PMI(t, c) = log --------------\n",
    "#                   P(t) P(c)\n",
    "#\n",
    "#    #(t, c): the co-occurrence count of t with c\n",
    "#    #(_, _): the sum of counts in the whole table, across all targets\n",
    "#    #(t, _): the sum of counts in the row of target t\n",
    "#    #(_, c): the sum of counts in the column of context item c\n",
    "#\n",
    "# then\n",
    "# P(t, c) = #(t, c) / #(_, _)\n",
    "# P(t) = #(t, _) / #(_, _)\n",
    "# P(c) = #(_, c) / #(_, _)\n",
    "#\n",
    "# PPMI(t, c) = { PMI(t, c) if PMI(t, c) >= 0\n",
    "#                0, else\n",
    "\n",
    "# target count #(t, _):\n",
    "print(\"target count for Sam\", sam_count_matrix.loc[\"Sam\"].sum())\n",
    "\n",
    "# overall count #(_, _):\n",
    "print(\"overall count\", sum(sam_count_matrix.sum()))\n",
    "\n",
    "# context item count #(_, c):\n",
    "print(\"context item count for eggs\", sam_count_matrix[\"eggs\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>I</th>\n",
       "      <th>Sam</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>do</th>\n",
       "      <th>eggs</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>like</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.134980</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.540445</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>1.252763</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.252763</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>1.134980</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0.847298</td>\n",
       "      <td>1.252763</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.252763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.94591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>1.540445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.94591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.94591</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               .         I       Sam        am      and        do     eggs  \\\n",
       "target                                                                       \n",
       ".       0.000000  0.000000  1.134980  0.847298  0.00000  0.000000  0.00000   \n",
       "I       0.000000  0.000000  0.847298  1.252763  0.00000  1.252763  0.00000   \n",
       "Sam     1.134980  0.847298  0.000000  0.847298  0.00000  0.000000  0.00000   \n",
       "am      0.847298  1.252763  0.847298  0.000000  0.00000  0.000000  0.00000   \n",
       "and     0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  1.94591   \n",
       "do      0.000000  1.252763  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "eggs    0.000000  0.000000  0.000000  0.000000  1.94591  0.000000  0.00000   \n",
       "green   0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  1.94591   \n",
       "ham     1.540445  0.000000  0.000000  0.000000  1.94591  0.000000  0.00000   \n",
       "like    0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \n",
       "not     0.000000  0.000000  0.000000  0.000000  0.00000  1.945910  0.00000   \n",
       "\n",
       "          green       ham     like      not  \n",
       "target                                       \n",
       ".       0.00000  1.540445  0.00000  0.00000  \n",
       "I       0.00000  0.000000  0.00000  0.00000  \n",
       "Sam     0.00000  0.000000  0.00000  0.00000  \n",
       "am      0.00000  0.000000  0.00000  0.00000  \n",
       "and     0.00000  1.945910  0.00000  0.00000  \n",
       "do      0.00000  0.000000  0.00000  1.94591  \n",
       "eggs    1.94591  0.000000  0.00000  0.00000  \n",
       "green   0.00000  0.000000  1.94591  0.00000  \n",
       "ham     0.00000  0.000000  0.00000  0.00000  \n",
       "like    1.94591  0.000000  0.00000  1.94591  \n",
       "not     0.00000  0.000000  1.94591  0.00000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# we'll store the association weights in a dictionary for now\n",
    "sam_pmi = { }\n",
    "\n",
    "count_all = sum(sam_count_matrix.sum())\n",
    "\n",
    "for target in sam_count_matrix.index:\n",
    "    for context in sam_count_matrix.columns:\n",
    "        p_t_c = sam_count_matrix.loc[target][context] / count_all\n",
    "        # print(\"p_t_c\", target, context, p_t_c)\n",
    "        p_t = sam_count_matrix.loc[target].sum() / count_all\n",
    "        # print(\"p_t\", target, p_t)\n",
    "        p_c = sam_count_matrix[context].sum() / count_all\n",
    "        # print(\"p_c\", context, p_c)\n",
    "\n",
    "        # we need to watch out: if p_t_c is zero, the logarithm is undefined\n",
    "        if p_t_c == 0.0 or p_t == 0.0 or p_c == 0.0:\n",
    "            pmi = 0.0\n",
    "        else: \n",
    "            pmi = math.log( p_t_c / (p_t * p_c))\n",
    "        \n",
    "        # print(\"pmi\", target, context, pmi)\n",
    "        \n",
    "        if target not in sam_pmi: sam_pmi[ target] = { }\n",
    "        sam_pmi[target][context]= pmi\n",
    "\n",
    "# And we again store the result in a data frame\n",
    "rows = [sam_pmi[t] for t in targetlist]\n",
    "sam_pmi_matrix = pd.DataFrame(rows).fillna(0)\n",
    "# set the target as the index\n",
    "sam_pmi_matrix[\"target\"] = targetlist\n",
    "sam_pmi_matrix.set_index(\"target\", inplace = True)\n",
    "# and reorder columns to match our canonical column order\n",
    "sam_pmi_matrix = sam_pmi_matrix.reindex(columns = contextlist, copy = False)\n",
    "sam_pmi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for dot:\n",
      "\t .        0.0\n",
      "I        0.0\n",
      "Sam      1.0\n",
      "am       1.0\n",
      "and      0.0\n",
      "do       0.0\n",
      "eggs     0.0\n",
      "green    0.0\n",
      "ham      1.0\n",
      "like     0.0\n",
      "not      0.0\n",
      "Name: ., dtype: float64\n",
      "Associations for dot:\n",
      "\t .        0.000000\n",
      "I        0.000000\n",
      "Sam      1.134980\n",
      "am       0.847298\n",
      "and      0.000000\n",
      "do       0.000000\n",
      "eggs     0.000000\n",
      "green    0.000000\n",
      "ham      1.540445\n",
      "like     0.000000\n",
      "not      0.000000\n",
      "Name: ., dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# We can directly compare counts and associations\n",
    "print(\"Counts for dot:\")\n",
    "print(\"\\t\", sam_count_matrix.loc[\".\"])\n",
    "print(\"Associations for dot:\")\n",
    "print(\"\\t\", sam_pmi_matrix.loc[\".\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine of 'I' and 'Sam', count-based 0.4714045207910318\n",
      "Cosine of 'I' and 'Sam', pmi-based 0.3274843356832646\n"
     ]
    }
   ],
   "source": [
    "# How has that changed cosines?\n",
    "print(\"Cosine of 'I' and 'Sam', count-based\", \n",
    "      cosine_sim(sam_count_matrix.loc[\"I\"], sam_count_matrix.loc[\"Sam\"]))\n",
    "print(\"Cosine of 'I' and 'Sam', pmi-based\", \n",
    "      cosine_sim(sam_pmi_matrix.loc[\"I\"], sam_pmi_matrix.loc[\"Sam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.49, 0.21, 0.27, 0.52, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.49, 1.  , 0.33, 0.21, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.45],\n",
       "       [0.21, 0.33, 1.  , 0.71, 0.  , 0.28, 0.  , 0.  , 0.43, 0.  , 0.  ],\n",
       "       [0.27, 0.21, 0.71, 1.  , 0.  , 0.39, 0.  , 0.  , 0.3 , 0.  , 0.  ],\n",
       "       [0.52, 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.28, 0.39, 0.  , 1.  , 0.  , 0.  , 0.  , 0.59, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.55, 0.5 , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 1.  , 0.  , 0.  , 0.5 ],\n",
       "       [0.  , 0.  , 0.43, 0.3 , 0.  , 0.  , 0.55, 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.59, 0.5 , 0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.45, 0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 1.  ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is how to get all pairwise cosines in our matrix:\n",
    "\n",
    "# Step 1: this computes pairwise cosine *distances*, 1 - cosine\n",
    "sam_cosine_dist = scipy.spatial.distance.cdist(sam_pmi_matrix, sam_pmi_matrix, metric = \"cosine\")\n",
    "\n",
    "# Step 2: convert to cosine similarity\n",
    "sam_cosine_sim = 1 - sam_cosine_dist\n",
    "\n",
    "# Let's look at this. sam_cosine_sim is a numpy ndarray, which\n",
    "# has a method round() for rounding.\n",
    "# As you can see, the cosine similarity of each word with itself is 1,\n",
    "# and the value for \"i\" and \"sam\", row 2, column 3, is 0.33, as computed above.\n",
    "sam_cosine_sim.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest neighbors\n",
    "\n",
    "We want to know about a word's nearest neighbors. Computing this by hand is a major pain: You would have to compute all pairwise cosines, and then rummage through them to find the maximum. In our tiny Sam corpus, this is feasible, but not in a large corpus. Fortunatly scikit-learn has a function NearestNeighbors that can do the work for us. One downside: It does not know cosine similarity outright. \n",
    "\n",
    "First option: We give it the cosine distance function that we used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(metric=<function cosine at 0x7f386ae823a0>, n_neighbors=3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# we make a nearest-neighbors object and tell it we'll always want the 3 nearest neighbors at a time\n",
    "nearest_neighbors_obj = NearestNeighbors(n_neighbors=3, metric = scipy.spatial.distance.cosine)\n",
    "\n",
    "# we then allow it to compute an internal datastructure from our data\n",
    "nearest_neighbors_obj.fit(sam_pmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([sam_pmi_matrix.loc[\"Sam\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor is Sam with similarity 1.0\n",
      "Neighbor is am with similarity 0.707098356669986\n",
      "Neighbor is ham with similarity 0.42683129808958764\n"
     ]
    }
   ],
   "source": [
    "# cosine_distances and target_indices are both two-dimensional arrays. Let's extract \n",
    "# lists of values\n",
    "cosine_distances = cosine_distances[0].tolist()\n",
    "target_indices = target_indices[0].tolist()\n",
    "\n",
    "for cosinedist, targetindex in zip(cosine_distances, target_indices):\n",
    "    print(\"Neighbor is\", targetlist[targetindex], \"with similarity\", 1 - cosinedist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Second option: We use Euclidean distance (walking distance), but first normalize all vectors to be of length one. This won't give us the same distance values, but the same orderings of what is nearest. See https://stackoverflow.com/questions/34144632/using-cosine-distance-with-scikit-learn-kneighborsclassifier\n",
    "\n",
    "Recall that the length of a vector is defined as \n",
    "\n",
    "$||a|| = \\sqrt{\\sum_i a_i^2}$\n",
    "\n",
    "The Python package numpy has an implementation of that. For a vector `a`, we need to call `numpy.linalg.norm(a, ord = 2)`.\n",
    "\n",
    "Note: The sum of values of a vector is called its \"L1 norm\", and the vector length is called its \"L2 norm\". That's why we need the parameter `ord = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>I</th>\n",
       "      <th>Sam</th>\n",
       "      <th>am</th>\n",
       "      <th>and</th>\n",
       "      <th>do</th>\n",
       "      <th>eggs</th>\n",
       "      <th>green</th>\n",
       "      <th>ham</th>\n",
       "      <th>like</th>\n",
       "      <th>not</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542372</td>\n",
       "      <td>0.404898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.736132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431445</td>\n",
       "      <td>0.637909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.637909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>0.687676</td>\n",
       "      <td>0.513372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.513372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>0.488761</td>\n",
       "      <td>0.722652</td>\n",
       "      <td>0.488761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>do</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eggs</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>green</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.620686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              .         I       Sam        am       and        do      eggs  \\\n",
       ".      0.000000  0.000000  0.542372  0.404898  0.000000  0.000000  0.000000   \n",
       "I      0.000000  0.000000  0.431445  0.637909  0.000000  0.637909  0.000000   \n",
       "Sam    0.687676  0.513372  0.000000  0.513372  0.000000  0.000000  0.000000   \n",
       "am     0.488761  0.722652  0.488761  0.000000  0.000000  0.000000  0.000000   \n",
       "and    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107   \n",
       "do     0.000000  0.541314  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "eggs   0.000000  0.000000  0.000000  0.000000  0.707107  0.000000  0.000000   \n",
       "green  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107   \n",
       "ham    0.620686  0.000000  0.000000  0.000000  0.784059  0.000000  0.000000   \n",
       "like   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "not    0.000000  0.000000  0.000000  0.000000  0.000000  0.707107  0.000000   \n",
       "\n",
       "          green       ham      like       not  \n",
       ".      0.000000  0.736132  0.000000  0.000000  \n",
       "I      0.000000  0.000000  0.000000  0.000000  \n",
       "Sam    0.000000  0.000000  0.000000  0.000000  \n",
       "am     0.000000  0.000000  0.000000  0.000000  \n",
       "and    0.000000  0.707107  0.000000  0.000000  \n",
       "do     0.000000  0.000000  0.000000  0.840820  \n",
       "eggs   0.707107  0.000000  0.000000  0.000000  \n",
       "green  0.000000  0.000000  0.707107  0.000000  \n",
       "ham    0.000000  0.000000  0.000000  0.000000  \n",
       "like   0.707107  0.000000  0.000000  0.707107  \n",
       "not    0.000000  0.000000  0.707107  0.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rows = [ sam_pmi_matrix.loc[t] / \n",
    "        np.linalg.norm(sam_pmi_matrix.loc[t], ord = 2)\n",
    "        for t in targetlist ]\n",
    "   \n",
    "sam_pminorm_matrix = pd.DataFrame(rows)\n",
    "\n",
    "sam_pminorm_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make a nearest-neighbors object \n",
    "# and tell it we'll always want the 2 nearest neighbors at a time.\n",
    "# Distance metric is default: \n",
    "# minkowski with p=2, which is equivalent to Euclidean distance\n",
    "nearest_neighbors_obj_2 = NearestNeighbors(n_neighbors=3)\n",
    "\n",
    "# we then allow it to compute an internal datastructure from our data\n",
    "nearest_neighbors_obj_2.fit(sam_pminorm_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor is Sam with distance 0.6504565357441716\n",
      "Neighbor is am with distance 1.1789557107969613\n",
      "Neighbor is ham with distance 1.521536646024799\n"
     ]
    }
   ],
   "source": [
    "# and let's look at nearest neighbors again\n",
    "distances, target_indices = nearest_neighbors_obj_2.kneighbors([sam_pmi_matrix.loc[\"Sam\"]])\n",
    "\n",
    "# cosine_distances and target_indices are both two-dimensional arrays. Let's extract \n",
    "# lists of values\n",
    "distances = distances[0].tolist()\n",
    "target_indices = target_indices[0].tolist()\n",
    "\n",
    "for dist, targetindex in zip(distances, target_indices):\n",
    "    print(\"Neighbor is\", targetlist[targetindex], \"with distance\", dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using a somewhat larger corpus\n",
    "\n",
    "We next demonstrate a somewhat larger corpus, with yet another method of accessing the corpus data: If the data is available within the NLTK corpora, you can use the NLTK's corpus reader to access it.\n",
    "\n",
    "The Brown corpus is a 1 million word corpus of carefully selected text pieces from different genres, originally made to support dictionary-makers, so it's intended to cover a broad variety of genres in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few sentences of the Brown corpus:\n",
      "\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'] \n",
      "\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'] \n",
      "\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The first few sentences of the Brown corpus:\\n\")\n",
    "for s in nltk.corpus.brown.sents()[:3]: \n",
    "    print(s, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We compute the target/context counts, noting context items as we go. We only count words that appear at least 10 times in the corpus. This cuts down a lot on the size of our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011),\n",
       " ('was', 9777),\n",
       " ('for', 8841),\n",
       " ('``', 8837),\n",
       " (\"''\", 8789),\n",
       " ('The', 7258),\n",
       " ('with', 7012),\n",
       " ('it', 6723),\n",
       " ('as', 6706),\n",
       " ('he', 6566),\n",
       " ('his', 6466)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_wordcounts = nltk.FreqDist(nltk.corpus.brown.words())\n",
    "brown_wordcounts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "brown_context_counts = nltk.ConditionalFreqDist()\n",
    "\n",
    "frequency_threshold = 20\n",
    "\n",
    "for sentence in nltk.corpus.brown.sents():\n",
    "    # remove punctuation.\n",
    "    # at this point you could also remove stopwords\n",
    "    # or iterate over sents_tagged() instead of sents()\n",
    "    # to get parts of speech, and only retain\n",
    "    # content words\n",
    "    wordlist = [w for w in sentence if w.strip(string.punctuation) != \"\"]\n",
    "    \n",
    "    for targetindex, target in enumerate(wordlist):\n",
    "        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n",
    "            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n",
    "                brown_context_counts[target][contextword] += 1   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this larger corpus, it now makes sense to look at some context word counts to get a sense of what the tables of counts tell us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent contexts for some targets:\n",
      "\n",
      "election:\n",
      " [('the', 21), ('of', 11), ('to', 6), ('for', 4), ('was', 3), ('and', 3), ('his', 3), ('I', 3), ('on', 3), ('in', 3)]\n",
      "love:\n",
      " [('of', 36), ('and', 34), ('in', 26), ('for', 22), ('to', 21), ('the', 17), ('with', 14), ('I', 13), ('you', 10), ('is', 10)]\n",
      "car: [('the', 84), ('a', 36), ('and', 19), ('his', 18), ('was', 13), ('with', 13), ('The', 10), ('is', 8), ('police', 7), ('that', 6)]\n"
     ]
    }
   ],
   "source": [
    "# 10 most frequent context words: similar across many items\n",
    "# (what can we do about that?)\n",
    "print(\"10 most frequent contexts for some targets:\\n\")\n",
    "print(\"election:\\n\", brown_context_counts[\"election\"].most_common(10))\n",
    "print(\"love:\\n\", brown_context_counts[\"love\"].most_common(10))\n",
    "print(\"car:\", brown_context_counts[\"car\"].most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most frequent contexts for some targets:\n",
      "\n",
      "election:\n",
      " [('the', 21), ('of', 11), ('to', 6), ('for', 4), ('was', 3), ('and', 3), ('his', 3), ('I', 3), ('on', 3), ('in', 3), ('an', 3), ('primary', 2), ('campaign', 2), ('last', 2), ('Presidential', 2), ('results', 2), ('November', 2), ('is', 2), ('April', 2), ('year', 2), ('produced', 1), ('laws', 1), ('general', 1), ('orderly', 1), ('8', 1), ('were', 1), ('investigation', 1), ('day', 1), ('told', 1), ('possible', 1), ('special', 1), ('might', 1), ('did', 1), (\"you'll\", 1), ('received', 1), ('they', 1), ('into', 1), ('bond', 1), ('The', 1), ('will', 1), ('Board', 1), ('Thursday', 1), ('His', 1), ('national', 1), ('close', 1), ('over', 1), ('procedures', 1), ('that', 1), ('inspired', 1), ('missed', 1), ('as', 1), ('falls', 1), ('law', 1), ('dates', 1), ('what', 1), ('plans', 1), ('may', 1)]\n",
      "love:\n",
      " [('of', 36), ('and', 34), ('in', 26), ('for', 22), ('to', 21), ('the', 17), ('with', 14), ('I', 13), ('you', 10), ('is', 10), ('that', 7), ('my', 6), ('it', 5), ('we', 5), ('as', 4), ('her', 4), ('this', 4), ('a', 4), ('him', 4), ('us', 3), ('not', 3), ('his', 3), ('but', 3), ('His', 3), ('only', 3), ('true', 3), (\"God's\", 2), ('through', 2), ('without', 2), ('Christian', 2), ('just', 2), ('them', 2), ('was', 2), ('songs', 2), (\"I'd\", 2), ('by', 2), ('romantic', 2), ('My', 2), ('made', 2), ('could', 2), ('can', 1), ('Eisenhower', 1), ('boys', 1), ('children', 1), ('somebody', 1), ('little', 1), (\"mother's\", 1), ('first', 1), ('light', 1), ('deep', 1), ('wisdom', 1), ('common', 1), ('God', 1), ('men', 1), ('earlier', 1), ('people', 1), ('They', 1), ('mutual', 1), ('who', 1), ('great', 1), ('he', 1), ('shade', 1), ('poems', 1), ('another', 1), ('lies', 1), ('force', 1), ('inspired', 1), ('If', 1), ('reflects', 1), ('then', 1), ('on', 1), ('has', 1), ('are', 1), ('impossible', 1), ('nor', 1), ('reality', 1), ('This', 1), ('belongs', 1), ('whose', 1), ('lost', 1), ('more', 1), ('memory', 1), ('letters', 1), ('about', 1), ('knowledge', 1), ('discovered', 1), ('story', 1), ('which', 1), ('song', 1), (\"didn't\", 1), ('they', 1), ('Lord', 1), ('all', 1), ('among', 1), ('way', 1), ('honor', 1), ('she', 1), ('bringing', 1), ('faith', 1), ('would', 1)]\n",
      "car:\n",
      " [('the', 84), ('a', 36), ('and', 19), ('his', 18), ('was', 13), ('with', 13), ('The', 10), ('is', 8), ('police', 7), ('that', 6), ('in', 6), ('big', 5), ('motor', 5), ('your', 5), ('parked', 5), ('my', 5), ('to', 4), ('he', 4), ('driven', 4), ('little', 4), ('of', 4), ('second', 4), ('had', 4), ('other', 3), ('new', 3), ('any', 3), ('her', 3), ('but', 3), ('by', 3), ('sports', 3), ('as', 3), ('could', 3), ('will', 3), ('for', 3), ('coming', 3), ('I', 3), ('on', 3), ('passing', 2), ('sales', 2), ('itself', 2), ('which', 2), ('speed', 2), ('box', 2), ('another', 2), ('at', 2), ('before', 2), ('industry', 2), ('until', 2), ('freight', 2), ('she', 2), ('into', 2), ('agency', 2), ('their', 2), ('door', 2), ('approaching', 2), ('here', 2), ('patrol', 2), ('own', 2), ('it', 2), ('old', 2), ('Street', 1), ('apart', 1), ('flying', 1), ('family', 1), ('throw', 1), (\"mother's\", 1), ('wanted', 1), ('heading', 1), ('number', 1), ('when', 1), ('they', 1), ('evidently', 1), ('used', 1), ('brushed', 1), ('New', 1), ('wrong', 1), ('now', 1), ('one', 1), ('herself', 1), (\"they'd\", 1), (\"isn't\", 1), ('foreign', 1), ('you', 1), ('coat', 1), ('My', 1), ('like', 1), ('Army', 1), ('smaller', 1), ('free', 1), ('back', 1), ('private', 1), ('driving', 1), ('must', 1), ('determine', 1), ('much', 1), ('buying', 1), ('covering', 1), ('more', 1), ('model', 1), ('might', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 100 most frequent context words: now we are starting to see differences.\n",
    "# We also see that many of the 100 most frequent context words only have counts of one.\n",
    "print(\"100 most frequent contexts for some targets:\\n\")\n",
    "print(\"election:\\n\", brown_context_counts[\"election\"].most_common(100))\n",
    "print(\"love:\\n\", brown_context_counts[\"love\"].most_common(100))\n",
    "print(\"car:\\n\", brown_context_counts[\"car\"].most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some ambiguous words:\n",
      "bat:\n",
      " []\n",
      "bank:\n",
      " [('the', 23), ('of', 9), ('a', 4), ('and', 4), ('in', 3), ('The', 2), ('local', 2), ('south', 2), ('which', 2), ('east', 2), ('west', 2), ('over', 1), ('accounts', 1), ('customers', 1), ('have', 1), ('That', 1), ('installed', 1), ('said', 1), ('is', 1), ('policy', 1), ('cloud', 1), ('that', 1), ('would', 1), ('for', 1), ('handling', 1), ('president', 1), ('to', 1), ('with', 1), ('officials', 1), ('before', 1), ('by', 1), ('loans', 1), ('left', 1), ('wrong', 1), ('far', 1), ('soft', 1), ('toward', 1), ('through', 1), ('river', 1), ('high', 1), ('outside', 1), ('big', 1), ('roll', 1)]\n",
      "bar:\n",
      " [('the', 34), ('and', 10), ('locking', 10), ('a', 4), ('Af', 3), ('in', 3), ('while', 2), ('he', 2), ('to', 2), ('patent', 2), ('Would', 1), ('vehicles', 1), ('without', 1), ('cocktail', 1), ('which', 1), ('come', 1), ('at', 1), ('held', 1), ('A', 1), ('stock', 1), ('on', 1), ('should', 1), ('our', 1), ('is', 1), ('as', 1), ('who', 1), ('with', 1), ('so', 1), ('absolute', 1), ('not', 1), ('finding', 1), ('little', 1), ('next', 1), ('was', 1), ('breakfast', 1), ('kitchen', 1), ('off', 1), ('headed', 1), ('same', 1), ('some', 1), ('if', 1), ('windows', 1), ('running', 1), ('but', 1), ('top', 1), ('crowded', 1)]\n",
      "leave:\n",
      " [('to', 79), ('the', 41), ('and', 17), ('it', 14), ('you', 10), ('him', 9), ('I', 8), ('not', 8), ('her', 8), ('his', 6), ('a', 6), ('this', 5), ('they', 4), ('my', 4), ('for', 4), ('France', 4), ('me', 4), ('here', 3), ('will', 3), ('with', 3), ('of', 3), ('but', 3), ('on', 3), ('their', 3), ('or', 3), ('just', 3), (\"can't\", 3), ('behind', 2), ('we', 2), ('us', 2), ('them', 2), ('because', 2), ('in', 2), ('at', 2), (\"couldn't\", 2), ('cannot', 2), ('from', 2), ('better', 2), ('never', 2), ('before', 2), ('all', 2), ('that', 2), (\"wouldn't\", 2), (\"he'd\", 2), ('could', 2), (\"I'll\", 2), ('one', 2), ('Eisenhower', 1), ('Capitol', 1), ('Sunday', 1), ('have', 1), ('both', 1), ('can', 1), ('until', 1), ('Parker', 1), ('clay', 1), ('seldom', 1), ('no', 1), ('some', 1), ('received', 1), ('he', 1), ('simply', 1), ('earlier', 1), ('through', 1), ('people', 1), ('out', 1), ('ever', 1), ('room', 1), ('replied', 1), ('Mr.', 1), ('may', 1), ('London', 1), ('therefore', 1), ('our', 1), ('everything', 1), ('Rome', 1), ('General', 1), ('Under', 1), ('administrative', 1), ('then', 1), ('which', 1), ('is', 1), ('dominant', 1), ('trying', 1), ('also', 1), ('1', 1), ('should', 1), ('New', 1), ('take', 1), ('must', 1), ('off', 1), ('only', 1), ('need', 1), ('would', 1), ('But', 1), ('around', 1), ('your', 1), (\"don't\", 1), ('than', 1), (\"'em\", 1)]\n"
     ]
    }
   ],
   "source": [
    "# some ambiguous words\n",
    "print(\"Some ambiguous words:\")\n",
    "print(\"bat:\\n\", brown_context_counts[\"bat\"].most_common(100))\n",
    "print(\"bank:\\n\", brown_context_counts[\"bank\"].most_common(100))\n",
    "print(\"bar:\\n\", brown_context_counts[\"bar\"].most_common(100))\n",
    "print(\"leave:\\n\", brown_context_counts[\"leave\"].most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with numpy matrices\n",
    "\n",
    "We re-compute the whole space as a matrix, this time using numpy, because this corpus is already too big to fit comfortably into pandas. (Meaning, it takes forever to compute the dataframes.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first determine the list of all words in Brown\n",
    "# repeat: frequency threshold\n",
    "frequency_threshold = 20\n",
    "\n",
    "brown_wordlist = list(w for w in brown_wordcounts if brown_wordcounts[w] >= frequency_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that maps each word to its index in the wordlist\n",
    "brown_wordlist_lookup = dict((word, index) for index, word in enumerate(brown_wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We need an array with enough space to hold \n",
    "# len(brown_wordlist) target words, and\n",
    "# len(brown_wordlist) context words.\n",
    "# We first initialize it to all zeros.\n",
    "brown_count_matrix = np.zeros((len(brown_wordlist), len(brown_wordlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's do the context word counting with this matrix.\n",
    "\n",
    "import string\n",
    "\n",
    "for sentence in nltk.corpus.brown.sents():\n",
    "    # remove punctuation.\n",
    "    # at this point you could also remove stopwords\n",
    "    # or iterate over sents_tagged() instead of sents()\n",
    "    # to get parts of speech, and only retain\n",
    "    # content words\n",
    "    wordlist = [w for w in sentence if w.strip(string.punctuation) != \"\"]\n",
    "    \n",
    "    for targetindex, target in enumerate(wordlist):\n",
    "        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n",
    "            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n",
    "                # which cell in the matrix is this? \n",
    "                # look up both the target and the context word\n",
    "                # in the ordered list of Brown words\n",
    "                targetindex_matrix = brown_wordlist_lookup[target]\n",
    "                contextindex_matrix = brown_wordlist_lookup[contextword]\n",
    "                # and add a count of one for this cell in the matrix\n",
    "                brown_count_matrix[targetindex_matrix][contextindex_matrix] += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8750717787803817"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can again compute similarity in this space\n",
    "said_index = brown_wordlist_lookup[\"said\"]\n",
    "wrote_index = brown_wordlist_lookup[\"wrote\"]\n",
    "cosine_sim( brown_count_matrix[said_index], brown_count_matrix[wrote_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now compute PMI again, making use of the fact that\n",
    "\n",
    "$PMI(a, b) = \\log \\frac{P(a, b)}{P(a)P(b)} = \\log\\frac{P(b|a)}{P(b)}$\n",
    "\n",
    "numpy offers functions that apply an operation to a whole vector at once,\n",
    "rather than one at a time. \n",
    "This is much quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can compute PMI again.\n",
    "# numpy offers functions that\n",
    "# apply an operation to a whole vector at once,\n",
    "# rather than one at a time. \n",
    "# This is much quicker.\n",
    "\n",
    "count_all = brown_count_matrix.sum()\n",
    "\n",
    "# probability of contexts/columns:\n",
    "# this is our P(b)\n",
    "# This is a vector with a probability for each context\n",
    "# sum(axis=0) is numpy's way of saying that we want to sum each column\n",
    "col_totals = brown_count_matrix.sum(axis=0)\n",
    "# avoid zeros, they get us in trouble later when we divide by p_c\n",
    "col_totals[col_totals == 0] = 0.00001\n",
    "# this is a vector where each row total is divided by the overall count\n",
    "p_c = col_totals / count_all\n",
    "\n",
    "# probability of context given target:\n",
    "# this is our P(b|a)\n",
    "# sum(axis=1) is numpy's way of saying that we want to sum each row:\n",
    "# we divide each row by its row total, getting the probability of a context item\n",
    "# within this target. \n",
    "# do do this, we flip the matrix on its side so that columns become rows,\n",
    "# then do the division (otherwise numpy would do column-wise instead of row-wise division)\n",
    "row_totals = brown_count_matrix.sum(axis=1).astype(float)\n",
    "row_totals[row_totals == 0] = 0.00001\n",
    "p_c_given_t = (brown_count_matrix.T / row_totals).T\n",
    "\n",
    "# PMI: log( P(b|a) / P(b))\n",
    "# we again divide a matrix by a vector\n",
    "# this time we do want column-wise division\n",
    "# so we don't have to flip the matrix\n",
    "pct_divided_by_pc = p_c_given_t / p_c\n",
    "# avoid doing log(0)by replacing 0 by a small number\n",
    "pct_divided_by_pc[pct_divided_by_pc==0] = 0.00001\n",
    "\n",
    "brown_pmi_matrix = np.log(pct_divided_by_pc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9244136837103797"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and computing similarity again\n",
    "said_index = brown_wordlist_lookup[\"said\"]\n",
    "wrote_index = brown_wordlist_lookup[\"wrote\"]\n",
    "cosine_sim( brown_pmi_matrix[said_index], brown_pmi_matrix[wrote_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(metric=<function cosine at 0x7f386ae823a0>, n_neighbors=20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors_obj = NearestNeighbors(n_neighbors=20, metric = scipy.spatial.distance.cosine)\n",
    "\n",
    "# we then allow it to compute an internal datastructure from our data\n",
    "nearest_neighbors_obj.fit(brown_pmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([brown_pmi_matrix[said_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "saidNN = pd.DataFrame(\n",
    "    np.c_[\n",
    "        np.array([brown_wordlist[i] for i in [target_indices][0][0]]), \n",
    "        cosine_distances[0]\n",
    "    ],\n",
    "    columns = ['word', 'cosine similarity']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cosine similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>told</td>\n",
       "      <td>0.06925978938395227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suspected</td>\n",
       "      <td>0.07028412566201891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>smiled</td>\n",
       "      <td>0.07041002420236053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>talked</td>\n",
       "      <td>0.07048890962242338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>remarked</td>\n",
       "      <td>0.07054217639994487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>impressed</td>\n",
       "      <td>0.07065588960128677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>happened</td>\n",
       "      <td>0.07068130870191813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>forgotten</td>\n",
       "      <td>0.07078041856535933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>crossed</td>\n",
       "      <td>0.07079818625325662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>laughed</td>\n",
       "      <td>0.07092304173412167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wondered</td>\n",
       "      <td>0.07093142453288204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>remember</td>\n",
       "      <td>0.07095636450053655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>asked</td>\n",
       "      <td>0.07102160672561997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fired</td>\n",
       "      <td>0.07110430028519388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>grabbed</td>\n",
       "      <td>0.07113687651011669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wonder</td>\n",
       "      <td>0.07117718106960946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>regarded</td>\n",
       "      <td>0.07117866695332009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>released</td>\n",
       "      <td>0.07117960861561234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>noticed</td>\n",
       "      <td>0.07119119891395131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word    cosine similarity\n",
       "0        said                  0.0\n",
       "1        told  0.06925978938395227\n",
       "2   suspected  0.07028412566201891\n",
       "3      smiled  0.07041002420236053\n",
       "4      talked  0.07048890962242338\n",
       "5    remarked  0.07054217639994487\n",
       "6   impressed  0.07065588960128677\n",
       "7    happened  0.07068130870191813\n",
       "8   forgotten  0.07078041856535933\n",
       "9     crossed  0.07079818625325662\n",
       "10    laughed  0.07092304173412167\n",
       "11   wondered  0.07093142453288204\n",
       "12   remember  0.07095636450053655\n",
       "13      asked  0.07102160672561997\n",
       "14      fired  0.07110430028519388\n",
       "15    grabbed  0.07113687651011669\n",
       "16     wonder  0.07117718106960946\n",
       "17   regarded  0.07117866695332009\n",
       "18   released  0.07117960861561234\n",
       "19    noticed  0.07119119891395131"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saidNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Creating a term-document matrix\n",
    "\n",
    "Data are taken from https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â../dataâ: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-01 12:06:00--  https://unitc-my.sharepoint.com/:x:/g/personal/nwsja01_cloud_uni-tuebingen_de/ESJ8QyqzmxpInK_jMQvQ_LEB9XoRbcw39rFmM6ZAQjrwvA?download=1\n",
      "Resolving unitc-my.sharepoint.com (unitc-my.sharepoint.com)... 13.107.138.9, 13.107.136.9\n",
      "Connecting to unitc-my.sharepoint.com (unitc-my.sharepoint.com)|13.107.138.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /personal/nwsja01_cloud_uni-tuebingen_de/Documents/vectorSpaceSemantics2022/data/Reviews.csv?ga=1 [following]\n",
      "--2022-05-01 12:06:00--  https://unitc-my.sharepoint.com/personal/nwsja01_cloud_uni-tuebingen_de/Documents/vectorSpaceSemantics2022/data/Reviews.csv?ga=1\n",
      "Reusing existing connection to unitc-my.sharepoint.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 300904694 (287M) [application/octet-stream]\n",
      "Saving to: â../data/Reviews.csvâ\n",
      "\n",
      "../data/Reviews.csv 100%[===================>] 286.96M  38.2MB/s    in 7.9s    \n",
      "\n",
      "2022-05-01 12:06:08 (36.2 MB/s) - â../data/Reviews.csvâ saved [300904694/300904694]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://unitc-my.sharepoint.com/:x:/g/personal/nwsja01_cloud_uni-tuebingen_de/ESJ8QyqzmxpInK_jMQvQ_LEB9XoRbcw39rFmM6ZAQjrwvA?download=1 -O ../data/Reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      5  1303862400   \n",
       "1                          0                       0      1  1346976000   \n",
       "2                          1                       1      4  1219017600   \n",
       "3                          3                       3      2  1307923200   \n",
       "4                          0                       0      5  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "568449                     0                       0      5  1299628800   \n",
       "568450                     0                       0      2  1331251200   \n",
       "568451                     2                       2      5  1329782400   \n",
       "568452                     1                       1      5  1331596800   \n",
       "568453                     0                       0      5  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[568454 rows x 10 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/Reviews.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python3_vss",
   "language": "python",
   "name": "python3_vss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
