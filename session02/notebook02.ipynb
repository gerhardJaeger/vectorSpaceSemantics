{"cells":[{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Vector space semantics\n\n## Session 02: Count-based models; dimensionality reduction\n\n### Gerhard JÃ¤ger\n\n\nMay 2, 2022\n\n(based on slides by Katrin Erk, https://www.katrinerk.com/courses/lin350-computational-semantics, with kind permission)"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Making a count-based distributional model\n\nYou need to make your own count-based distributional model if:\n* you want to study word contexts in a particular genre or text collection\n* you want to have an interpretable model where you can understand what the individual dimensions are\n* you want to compute a distributional model from an amount of data that is comparable to the amount of text a person reads (whoever reads the whole Wikipedia?)\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## A tiny space\n\nWe start with a tiny corpus, so that we can easily inspect our data: "},{"metadata":{"slideshow":{"slide_type":""},"trusted":true},"cell_type":"code","source":"sam_corpus = \"\"\"I am Sam.\nSam I am.\nI do not like green eggs and ham.\"\"\"\n","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here is how to split the corpus into sentences using the Natural Language Toolkit:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')","execution_count":2,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","name":"stderr"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nsam_sentences = nltk.sent_tokenize(sam_corpus)\nsam_sentences","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"['I am Sam.', 'Sam I am.', 'I do not like green eggs and ham.']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We need to get the context around a target word. To do this, we make a function that, for a target index, yields the words preceding and succeeding (if any). For simplicity, we use a one-word window on either side of the target. \n\n(It has \"yield\" instead of \"return\", so it is intended to be used in a for-loop.)"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"def each_contextword_1wordwindow(wordlist, targetindex):\n    if targetindex > 0:\n        # preceding word\n        yield wordlist[targetindex - 1]\n        \n    if targetindex < len(wordlist)- 1:\n        # succeeding word\n        yield wordlist[targetindex + 1]\n        ","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wl = ['a', 'b', 'c', 'd', 'e']\n\nfor x in each_contextword_1wordwindow(wl, 3):\n    print(x)\n","execution_count":5,"outputs":[{"output_type":"stream","text":"c\ne\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Here is how to use it: We count, for each target word, how often each context word appears with it. As an aside, first a few quick words about data structures in NLTK that support us in counting words (or word groups, or pieces of syntactic structure). The first is basically a dictionary mapping words to counts, called a FreqDist. Conveniently, you can just initialize it by giving it a list of items, and it will count how often each item appears in the list:"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# NLTK data structures for counting stuff:\n# count individual words or other items:\n\nfd = nltk.FreqDist([\"a\", \"b\", \"c\", \"a\", \"b\", \"d\"])\nfd","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"FreqDist({'a': 2, 'b': 2, 'c': 1, 'd': 1})"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The second data structure relevant for us today is the ConditionalFreqDist. It also has counts, but it can be used to count, for each target, how often each context word appears, or more generally, how often each word appears given some other word. Say \"a\" is a target, and \"b\" and \"c\" are context items, then a ConditionalFreqDist can be used like a two-deep dictionary, whose first-level keys are called \"conditions\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for targets, count context words,\n# or in general, for one sort of items, \n# count another sort of items\ncfd = nltk.ConditionalFreqDist()\ncfd[\"a\"][\"b\"] += 1\ncfd[\"a\"][\"c\"] += 1\ncfd","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"<ConditionalFreqDist with 1 conditions>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"For the \"condition\" 'a', the entry is again a FreqDist object that counts appearances of 'b' and 'c':"},{"metadata":{"trusted":true},"cell_type":"code","source":"cfd[\"a\"]","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"FreqDist({'b': 1, 'c': 1})"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"You can also initialize a ConditionalFreqDist by a list of pairs. It then counts, for each first item of the pair, how often each second item appears. In the next example, the ConditionalFreqDist will record that given \"a\", both \"b\" and \"c\" appeared once, and that given \"d\", \"e\" appeared once:"},{"metadata":{"trusted":true},"cell_type":"code","source":"cfd = nltk.ConditionalFreqDist([(\"a\", \"b\"), (\"a\", \"c\"), (\"d\", \"e\")])\ncfd","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<ConditionalFreqDist with 2 conditions>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cfd[\"a\"]","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"FreqDist({'b': 1, 'c': 1})"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now back to our Sam corpus. We can count context words for each target using a ConditionalFreqDist where the conditions are targets, and the keys of the FreqDist's are context words:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we will store the target words and their context word counts\n# this is a data type with method  conditions() to get the list of target words,\n# and sam_context_counts[t][cx] gets you the count for target t and context word cx\nsam_context_counts = nltk.ConditionalFreqDist()\n\n# iterate\nfor sentence in sam_sentences:\n    wordlist = nltk.word_tokenize(sentence)\n    \n    for targetindex, target in enumerate(wordlist):\n        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n            sam_context_counts[target][contextword] += 1\n","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here are the target words from our corpus\nsam_context_counts.conditions()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"['I', 'am', 'Sam', '.', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# Here are all the target/context counts we got from our corpus\nlist(sam_context_counts.items())","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"[('I', FreqDist({'am': 2, 'Sam': 1, 'do': 1})),\n ('am', FreqDist({'I': 2, 'Sam': 1, '.': 1})),\n ('Sam', FreqDist({'am': 1, '.': 1, 'I': 1})),\n ('.', FreqDist({'Sam': 1, 'am': 1, 'ham': 1})),\n ('do', FreqDist({'I': 1, 'not': 1})),\n ('not', FreqDist({'do': 1, 'like': 1})),\n ('like', FreqDist({'not': 1, 'green': 1})),\n ('green', FreqDist({'like': 1, 'eggs': 1})),\n ('eggs', FreqDist({'green': 1, 'and': 1})),\n ('and', FreqDist({'eggs': 1, 'ham': 1})),\n ('ham', FreqDist({'and': 1, '.': 1}))]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here are the counts for one target\nsam_context_counts[\"eggs\"]","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"FreqDist({'green': 1, 'and': 1})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here's an example of how to access one count \n# for target \"I\" and context \"do\"\nsam_context_counts[\"I\"][\"do\"]","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We put our rows and columns in order.\n# For now, our rows are the same as our columns:\n# All target words are also context words, and vice versa.\n# But that doesn't have to be the case.\ntargetlist = sorted(sam_context_counts.conditions())\ncontextlist = sorted(list(set(c for t in sam_context_counts.conditions() \n                              for c in sam_context_counts[t].keys())))\n\nprint(\"Targets\", targetlist)\nprint(\"Contexts\", contextlist)","execution_count":16,"outputs":[{"output_type":"stream","text":"Targets ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\nContexts ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# we can also put all our counts in a pandas dataframe\n\nimport pandas as pd\n\n# we turn our conditional frequency counts into a matrix,\n# filling the empty places (NaN) with zeros \nrows = [sam_context_counts[t] for t in targetlist]\nsam_count_matrix = pd.DataFrame(rows).fillna(0)\n# add row labels as column \"target\", and make it the index\nsam_count_matrix[\"target\"] = targetlist\nsam_count_matrix.set_index(\"target\", inplace = True)\n# reorder all other columns in the order of the context list\nsam_count_matrix = sam_count_matrix.reindex(columns = contextlist, copy = False)\nsam_count_matrix","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"          .    I  Sam   am  and   do  eggs  green  ham  like  not\ntarget                                                           \n.       0.0  0.0  1.0  1.0  0.0  0.0   0.0    0.0  1.0   0.0  0.0\nI       0.0  0.0  1.0  2.0  0.0  1.0   0.0    0.0  0.0   0.0  0.0\nSam     1.0  1.0  0.0  1.0  0.0  0.0   0.0    0.0  0.0   0.0  0.0\nam      1.0  2.0  1.0  0.0  0.0  0.0   0.0    0.0  0.0   0.0  0.0\nand     0.0  0.0  0.0  0.0  0.0  0.0   1.0    0.0  1.0   0.0  0.0\ndo      0.0  1.0  0.0  0.0  0.0  0.0   0.0    0.0  0.0   0.0  1.0\neggs    0.0  0.0  0.0  0.0  1.0  0.0   0.0    1.0  0.0   0.0  0.0\ngreen   0.0  0.0  0.0  0.0  0.0  0.0   1.0    0.0  0.0   1.0  0.0\nham     1.0  0.0  0.0  0.0  1.0  0.0   0.0    0.0  0.0   0.0  0.0\nlike    0.0  0.0  0.0  0.0  0.0  0.0   0.0    1.0  0.0   0.0  1.0\nnot     0.0  0.0  0.0  0.0  0.0  1.0   0.0    0.0  0.0   1.0  0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>.</th>\n      <th>I</th>\n      <th>Sam</th>\n      <th>am</th>\n      <th>and</th>\n      <th>do</th>\n      <th>eggs</th>\n      <th>green</th>\n      <th>ham</th>\n      <th>like</th>\n      <th>not</th>\n    </tr>\n    <tr>\n      <th>target</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>.</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>I</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Sam</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>am</th>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>do</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>eggs</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>green</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ham</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>not</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"Here is a row from our matrix, the vector for \"Sam\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"sam_count_matrix.loc[\"Sam\"]","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":".        1.0\nI        1.0\nSam      0.0\nam       1.0\nand      0.0\ndo       0.0\neggs     0.0\ngreen    0.0\nham      0.0\nlike     0.0\nnot      0.0\nName: Sam, dtype: float64"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Vector distances\n\n- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n- **Euclidean** distance $d_2(\\mathbf u,\\mathbf v)$\n    \n<img src=\"_img/distances01.svg\" width=\"600\">\n\n$$\n    d_2(\\mathbf u,\\mathbf v) = \\sqrt{\n        (u_1-v_1)^2 + \\cdots + (u_n-v_n)^2\n    }\n$$"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Vector distances\n\n- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n- **Manhattan** (âcity blockâ) distance $d_1(\\mathbf u,\\mathbf v)$    \n\n    \n<img src=\"_img/distances02.svg\" width=\"600\">\n\n$$\n    d_1(\\mathbf u,\\mathbf v) = \n        |u_1-v_1| + \\cdots + |u_n-v_n|\n$$\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Vector distances\n\n- **distance** between vectors $\\mathbf{u}, \\mathbf{v}\\in \\mathbb R^n$ $\\Rightarrow$ (dis)similarity\n    - $\\mathbf u = (u_1, \\ldots, u_n)'$\n    - $\\mathbf v = (v_1, \\ldots, v+_n)'$\n- both are extensions of the Minkowski **$p$-distance** $d_p(\\mathbf u, \\mathbf v)$ (for $p> 0$)\n\n    \n<img src=\"_img/distances02.svg\" width=\"600\">\n\n$$\n\\begin{aligned}\n    d_p(\\mathbf u,\\mathbf v) &= \n        (|u_1-v_1|^p + \\cdots + |u_n-v_n|^p)^{\\frac{1}{p}}\\\\\n    d_\\infty(\\mathbf u,\\mathbf v) &= \n        \\max_{i=1^n} |u_i-v_i|       \n\\end{aligned}\n$$\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### âcircleâ for different values of $p$\n\n<img src=\"_img/2D_unit_balls.svg.png\" width=\"3000\">"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## cosine similarity\n\n- length of a vector in count-based model depends on number of occurrences $\\Rightarrow$ uninteresting for our purposes\n- to counter this effect, people often *normalize* vectors\n- length normalization:\n    - replace $\\mathbf v$ by $\\frac{\\mathbf v}{\\|\\mathbf v\\|}$\n    - $\\|\\mathbf v\\|$ is the ânormâ (length) of the vector\n    $$\n    \\|\\mathbf v\\| = \\sqrt{v_1^2 + \\cdots + v_n^2}\n    $$\n- vectors of norm $=1$ are called **unit vectors**    \n- a convenient measure of the similarity between two unit vectors is the cosine of the angle between them\n\n<img src=\"_img/trigonometry.svg\" width=600>\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## cosine similarity\n\n- numerically: \n    - for unit vectors, the cosine of the angle $\\theta$ between vectors $\\mathbf u$ and $\\mathbf v$ equals the *inner product* (also called âdot productâ)\n    \n    $$\n    \\cos \\theta = \\langle \\mathbf u, \\mathbf v\\rangle = \\mathbf u \\cdot \\mathbf v = \\sum u_iv_i +\\cdots+u_nv_n\n    $$\n    - for vectors in general\n    $$\\cos\\theta = \\frac{\\mathbf u\\cdot\\mathbf v}{\\|\\mathbf u\\|\\|\\mathbf v\\|}$$"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## cosine similarity\n\nNow we can compute cosine similarity in our space. Python has a built-in function for this â with one catch: It computes 1 - cosine, as a distance. Here's how to get back to cosine similarity:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\n\ndef cosine_sim(vec1, vec2):\n    return 1 - scipy.spatial.distance.cosine(vec1, vec2)\n\ncosine_sim(sam_count_matrix.loc[\"I\"], sam_count_matrix.loc[\"Sam\"])\n\n","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"0.4714045207910318"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"\n\n## From word co-occurrence counts to association weights\n\nAs we discussed in class, raw frequency counts may not be what we want -- we don't need to know that all words co-occur a lot with \"the\" and \"a\". Even if we ditch stopwords, the frequency bias in the data may not be what we want: Do we need to know that all words co-occur a lot with \"said\"? \n\nSeveral methods have been developed for going from counts to association weights, including tf/idf and pointwise mutual information. Here, we demonstrate how to compute pointwise mutual information, defined as\n\n$PMI(a, b) = \\log \\frac{P(a, b)}{P(a)P(b)}$ \n\nIn the numerator, we have the joint probability of a *and* b. The formula compares this to the denominator, which has the product of the probability of a and the probability of b: If a and b were completely independent, had zero association, we would expect them to co-occur only by chance, that is, we would expect $P(a, b) = P(a)P(b)$. If $P(a, b)$ is larger than $P(a)P(b)$, then a and b are positively associated -- they co-occur more often than you would expect just from chance encounters. If $P(a, b)$ is smaller than $P(a)P(b)$, then a and b are negatively associated -- they really don't want to go together. \n\nIn practice, we are often not interested in negative associations, and only use positive ones. Then we get PPMI:\n\n\n$PPMI(a, b) = \\left\\{\\begin{array}{ll}PMI(a, b) & \\text{if } PMI(a, b) > 0\\\\\n0 & \\text{else}\n\\end{array}\\right.$\n"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"\n\n\n# Here are the pieces we need:\n# pointwise mutual information (PMI):\n#                    P(t, c)\n# PMI(t, c) = log --------------\n#                   P(t) P(c)\n#\n#    #(t, c): the co-occurrence count of t with c\n#    #(_, _): the sum of counts in the whole table, across all targets\n#    #(t, _): the sum of counts in the row of target t\n#    #(_, c): the sum of counts in the column of context item c\n#\n# then\n# P(t, c) = #(t, c) / #(_, _)\n# P(t) = #(t, _) / #(_, _)\n# P(c) = #(_, c) / #(_, _)\n#\n# PPMI(t, c) = { PMI(t, c) if PMI(t, c) >= 0\n#                0, else\n\n# target count #(t, _):\nprint(\"target count for Sam\", sam_count_matrix.loc[\"Sam\"].sum())\n\n# overall count #(_, _):\nprint(\"overall count\", sum(sam_count_matrix.sum()))\n\n# context item count #(_, c):\nprint(\"context item count for eggs\", sam_count_matrix[\"eggs\"].sum())","execution_count":20,"outputs":[{"output_type":"stream","text":"target count for Sam 3.0\noverall count 28.0\ncontext item count for eggs 2.0\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"import math\n\n# we'll store the association weights in a dictionary for now\nsam_pmi = { }\n\ncount_all = sum(sam_count_matrix.sum())\n\nfor target in sam_count_matrix.index:\n    for context in sam_count_matrix.columns:\n        p_t_c = sam_count_matrix.loc[target][context] / count_all\n        # print(\"p_t_c\", target, context, p_t_c)\n        p_t = sam_count_matrix.loc[target].sum() / count_all\n        # print(\"p_t\", target, p_t)\n        p_c = sam_count_matrix[context].sum() / count_all\n        # print(\"p_c\", context, p_c)\n\n        # we need to watch out: if p_t_c is zero, the logarithm is undefined\n        if p_t_c == 0.0 or p_t == 0.0 or p_c == 0.0:\n            pmi = 0.0\n        else: \n            pmi = math.log( p_t_c / (p_t * p_c))\n        \n        # print(\"pmi\", target, context, pmi)\n        \n        if target not in sam_pmi: sam_pmi[ target] = { }\n        sam_pmi[target][context]= pmi\n\n# And we again store the result in a data frame\nrows = [sam_pmi[t] for t in targetlist]\nsam_pmi_matrix = pd.DataFrame(rows).fillna(0)\n# set the target as the index\nsam_pmi_matrix[\"target\"] = targetlist\nsam_pmi_matrix.set_index(\"target\", inplace = True)\n# and reorder columns to match our canonical column order\nsam_pmi_matrix = sam_pmi_matrix.reindex(columns = contextlist, copy = False)\nsam_pmi_matrix","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"               .         I       Sam        am      and        do     eggs  \\\ntarget                                                                       \n.       0.000000  0.000000  1.134980  0.847298  0.00000  0.000000  0.00000   \nI       0.000000  0.000000  0.847298  1.252763  0.00000  1.252763  0.00000   \nSam     1.134980  0.847298  0.000000  0.847298  0.00000  0.000000  0.00000   \nam      0.847298  1.252763  0.847298  0.000000  0.00000  0.000000  0.00000   \nand     0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  1.94591   \ndo      0.000000  1.252763  0.000000  0.000000  0.00000  0.000000  0.00000   \neggs    0.000000  0.000000  0.000000  0.000000  1.94591  0.000000  0.00000   \ngreen   0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  1.94591   \nham     1.540445  0.000000  0.000000  0.000000  1.94591  0.000000  0.00000   \nlike    0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.00000   \nnot     0.000000  0.000000  0.000000  0.000000  0.00000  1.945910  0.00000   \n\n          green       ham     like      not  \ntarget                                       \n.       0.00000  1.540445  0.00000  0.00000  \nI       0.00000  0.000000  0.00000  0.00000  \nSam     0.00000  0.000000  0.00000  0.00000  \nam      0.00000  0.000000  0.00000  0.00000  \nand     0.00000  1.945910  0.00000  0.00000  \ndo      0.00000  0.000000  0.00000  1.94591  \neggs    1.94591  0.000000  0.00000  0.00000  \ngreen   0.00000  0.000000  1.94591  0.00000  \nham     0.00000  0.000000  0.00000  0.00000  \nlike    1.94591  0.000000  0.00000  1.94591  \nnot     0.00000  0.000000  1.94591  0.00000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>.</th>\n      <th>I</th>\n      <th>Sam</th>\n      <th>am</th>\n      <th>and</th>\n      <th>do</th>\n      <th>eggs</th>\n      <th>green</th>\n      <th>ham</th>\n      <th>like</th>\n      <th>not</th>\n    </tr>\n    <tr>\n      <th>target</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>.</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.134980</td>\n      <td>0.847298</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>1.540445</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>I</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.847298</td>\n      <td>1.252763</td>\n      <td>0.00000</td>\n      <td>1.252763</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>Sam</th>\n      <td>1.134980</td>\n      <td>0.847298</td>\n      <td>0.000000</td>\n      <td>0.847298</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>am</th>\n      <td>0.847298</td>\n      <td>1.252763</td>\n      <td>0.847298</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.00000</td>\n      <td>1.945910</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>do</th>\n      <td>0.000000</td>\n      <td>1.252763</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.94591</td>\n    </tr>\n    <tr>\n      <th>eggs</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.94591</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>green</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>ham</th>\n      <td>1.540445</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.94591</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.94591</td>\n    </tr>\n    <tr>\n      <th>not</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>1.945910</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.94591</td>\n      <td>0.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# We can directly compare counts and associations\nprint(\"Counts for dot:\")\nprint(\"\\t\", sam_count_matrix.loc[\".\"])\nprint(\"Associations for dot:\")\nprint(\"\\t\", sam_pmi_matrix.loc[\".\"])\n","execution_count":22,"outputs":[{"output_type":"stream","text":"Counts for dot:\n\t .        0.0\nI        0.0\nSam      1.0\nam       1.0\nand      0.0\ndo       0.0\neggs     0.0\ngreen    0.0\nham      1.0\nlike     0.0\nnot      0.0\nName: ., dtype: float64\nAssociations for dot:\n\t .        0.000000\nI        0.000000\nSam      1.134980\nam       0.847298\nand      0.000000\ndo       0.000000\neggs     0.000000\ngreen    0.000000\nham      1.540445\nlike     0.000000\nnot      0.000000\nName: ., dtype: float64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How has that changed cosines?\nprint(\"Cosine of 'I' and 'Sam', count-based\", \n      cosine_sim(sam_count_matrix.loc[\"I\"], sam_count_matrix.loc[\"Sam\"]))\nprint(\"Cosine of 'I' and 'Sam', pmi-based\", \n      cosine_sim(sam_pmi_matrix.loc[\"I\"], sam_pmi_matrix.loc[\"Sam\"]))","execution_count":23,"outputs":[{"output_type":"stream","text":"Cosine of 'I' and 'Sam', count-based 0.4714045207910318\nCosine of 'I' and 'Sam', pmi-based 0.3274843356832646\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# Here is how to get all pairwise cosines in our matrix:\n\n# Step 1: this computes pairwise cosine *distances*, 1 - cosine\nsam_cosine_dist = scipy.spatial.distance.cdist(sam_pmi_matrix, sam_pmi_matrix, metric = \"cosine\")\n\n# Step 2: convert to cosine similarity\nsam_cosine_sim = 1 - sam_cosine_dist\n\n# Let's look at this. sam_cosine_sim is a numpy ndarray, which\n# has a method round() for rounding.\n# As you can see, the cosine similarity of each word with itself is 1,\n# and the value for \"i\" and \"sam\", row 2, column 3, is 0.33, as computed above.\nsam_cosine_sim.round(2)","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"array([[1.  , 0.49, 0.21, 0.27, 0.52, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n       [0.49, 1.  , 0.33, 0.21, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.45],\n       [0.21, 0.33, 1.  , 0.71, 0.  , 0.28, 0.  , 0.  , 0.43, 0.  , 0.  ],\n       [0.27, 0.21, 0.71, 1.  , 0.  , 0.39, 0.  , 0.  , 0.3 , 0.  , 0.  ],\n       [0.52, 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.28, 0.39, 0.  , 1.  , 0.  , 0.  , 0.  , 0.59, 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.55, 0.5 , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 1.  , 0.  , 0.  , 0.5 ],\n       [0.  , 0.  , 0.43, 0.3 , 0.  , 0.  , 0.55, 0.  , 1.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.  , 0.  , 0.  , 0.59, 0.5 , 0.  , 0.  , 1.  , 0.  ],\n       [0.  , 0.45, 0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 1.  ]])"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Nearest neighbors\n\nWe want to know about a word's nearest neighbors. Computing this by hand is a major pain: You would have to compute all pairwise cosines, and then rummage through them to find the maximum. In our tiny Sam corpus, this is feasible, but not in a large corpus. Fortunatly scikit-learn has a function NearestNeighbors that can do the work for us. One downside: It does not know cosine similarity outright. \n\nFirst option: We give it the cosine distance function that we used above. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import NearestNeighbors\n\n# we make a nearest-neighbors object and tell it we'll always want the 3 nearest neighbors at a time\nnearest_neighbors_obj = NearestNeighbors(n_neighbors=3, metric = scipy.spatial.distance.cosine)\n\n# we then allow it to compute an internal datastructure from our data\nnearest_neighbors_obj.fit(sam_pmi_matrix)","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"NearestNeighbors(metric=<function cosine at 0x7f3439f60550>, n_neighbors=3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([sam_pmi_matrix.loc[\"Sam\"]])","execution_count":26,"outputs":[{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n  warnings.warn(\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cosine_distances and target_indices are both two-dimensional arrays. Let's extract \n# lists of values\ncosine_distances = cosine_distances[0].tolist()\ntarget_indices = target_indices[0].tolist()\n\nfor cosinedist, targetindex in zip(cosine_distances, target_indices):\n    print(\"Neighbor is\", targetlist[targetindex], \"with similarity\", 1 - cosinedist)","execution_count":27,"outputs":[{"output_type":"stream","text":"Neighbor is Sam with similarity 1.0\nNeighbor is am with similarity 0.707098356669986\nNeighbor is ham with similarity 0.42683129808958764\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"Second option: We use Euclidean distance (walking distance), but first normalize all vectors to be of length one. This won't give us the same distance values, but the same orderings of what is nearest. See https://stackoverflow.com/questions/34144632/using-cosine-distance-with-scikit-learn-kneighborsclassifier\n\nRecall that the length of a vector is defined as \n\n$||a|| = \\sqrt{\\sum_i a_i^2}$\n\nThe Python package numpy has an implementation of that. For a vector `a`, we need to call `numpy.linalg.norm(a, ord = 2)`.\n\nNote: The sum of values of a vector is called its \"L1 norm\", and the vector length is called its \"L2 norm\". That's why we need the parameter `ord = 2`."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nrows = [ sam_pmi_matrix.loc[t] / \n        np.linalg.norm(sam_pmi_matrix.loc[t], ord = 2)\n        for t in targetlist ]\n   \nsam_pminorm_matrix = pd.DataFrame(rows)\n\nsam_pminorm_matrix","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"              .         I       Sam        am       and        do      eggs  \\\n.      0.000000  0.000000  0.542372  0.404898  0.000000  0.000000  0.000000   \nI      0.000000  0.000000  0.431445  0.637909  0.000000  0.637909  0.000000   \nSam    0.687676  0.513372  0.000000  0.513372  0.000000  0.000000  0.000000   \nam     0.488761  0.722652  0.488761  0.000000  0.000000  0.000000  0.000000   \nand    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107   \ndo     0.000000  0.541314  0.000000  0.000000  0.000000  0.000000  0.000000   \neggs   0.000000  0.000000  0.000000  0.000000  0.707107  0.000000  0.000000   \ngreen  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107   \nham    0.620686  0.000000  0.000000  0.000000  0.784059  0.000000  0.000000   \nlike   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \nnot    0.000000  0.000000  0.000000  0.000000  0.000000  0.707107  0.000000   \n\n          green       ham      like       not  \n.      0.000000  0.736132  0.000000  0.000000  \nI      0.000000  0.000000  0.000000  0.000000  \nSam    0.000000  0.000000  0.000000  0.000000  \nam     0.000000  0.000000  0.000000  0.000000  \nand    0.000000  0.707107  0.000000  0.000000  \ndo     0.000000  0.000000  0.000000  0.840820  \neggs   0.707107  0.000000  0.000000  0.000000  \ngreen  0.000000  0.000000  0.707107  0.000000  \nham    0.000000  0.000000  0.000000  0.000000  \nlike   0.707107  0.000000  0.000000  0.707107  \nnot    0.000000  0.000000  0.707107  0.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>.</th>\n      <th>I</th>\n      <th>Sam</th>\n      <th>am</th>\n      <th>and</th>\n      <th>do</th>\n      <th>eggs</th>\n      <th>green</th>\n      <th>ham</th>\n      <th>like</th>\n      <th>not</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>.</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.542372</td>\n      <td>0.404898</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.736132</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>I</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.431445</td>\n      <td>0.637909</td>\n      <td>0.000000</td>\n      <td>0.637909</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Sam</th>\n      <td>0.687676</td>\n      <td>0.513372</td>\n      <td>0.000000</td>\n      <td>0.513372</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>am</th>\n      <td>0.488761</td>\n      <td>0.722652</td>\n      <td>0.488761</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>and</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>do</th>\n      <td>0.000000</td>\n      <td>0.541314</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.840820</td>\n    </tr>\n    <tr>\n      <th>eggs</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>green</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ham</th>\n      <td>0.620686</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.784059</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>like</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n    </tr>\n    <tr>\n      <th>not</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# we make a nearest-neighbors object \n# and tell it we'll always want the 2 nearest neighbors at a time.\n# Distance metric is default: \n# minkowski with p=2, which is equivalent to Euclidean distance\nnearest_neighbors_obj_2 = NearestNeighbors(n_neighbors=3)\n\n# we then allow it to compute an internal datastructure from our data\nnearest_neighbors_obj_2.fit(sam_pminorm_matrix)","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"NearestNeighbors(n_neighbors=3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and let's look at nearest neighbors again\ndistances, target_indices = nearest_neighbors_obj_2.kneighbors([sam_pmi_matrix.loc[\"Sam\"]])\n\n# cosine_distances and target_indices are both two-dimensional arrays. Let's extract \n# lists of values\ndistances = distances[0].tolist()\ntarget_indices = target_indices[0].tolist()\n\nfor dist, targetindex in zip(distances, target_indices):\n    print(\"Neighbor is\", targetlist[targetindex], \"with distance\", dist)","execution_count":30,"outputs":[{"output_type":"stream","text":"Neighbor is Sam with distance 0.6504565357441716\nNeighbor is am with distance 1.1789557107969613\nNeighbor is ham with distance 1.521536646024799\n","name":"stdout"},{"output_type":"stream","text":"/srv/conda/envs/notebook/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but NearestNeighbors was fitted with feature names\n  warnings.warn(\n","name":"stderr"}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# Using a somewhat larger corpus\n\nWe next demonstrate a somewhat larger corpus, with yet another method of accessing the corpus data: If the data is available within the NLTK corpora, you can use the NLTK's corpus reader to access it.\n\nThe Brown corpus is a 1 million word corpus of carefully selected text pieces from different genres, originally made to support dictionary-makers, so it's intended to cover a broad variety of genres in English."},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('brown')\nprint(\"The first few sentences of the Brown corpus:\\n\")\nfor s in nltk.corpus.brown.sents()[:3]: \n    print(s, \"\\n\")","execution_count":32,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/brown.zip.\n","name":"stderr"},{"output_type":"stream","text":"The first few sentences of the Brown corpus:\n\n['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'] \n\n['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'] \n\n['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.'] \n\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"We compute the target/context counts, noting context items as we go. We only count words that appear at least 10 times in the corpus. This cuts down a lot on the size of our matrix."},{"metadata":{"trusted":true},"cell_type":"code","source":"brown_wordcounts = nltk.FreqDist(nltk.corpus.brown.words())\nbrown_wordcounts.most_common(20)","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"[('the', 62713),\n (',', 58334),\n ('.', 49346),\n ('of', 36080),\n ('and', 27915),\n ('to', 25732),\n ('a', 21881),\n ('in', 19536),\n ('that', 10237),\n ('is', 10011),\n ('was', 9777),\n ('for', 8841),\n ('``', 8837),\n (\"''\", 8789),\n ('The', 7258),\n ('with', 7012),\n ('it', 6723),\n ('as', 6706),\n ('he', 6566),\n ('his', 6466)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\nbrown_context_counts = nltk.ConditionalFreqDist()\n\nfrequency_threshold = 20\n\nfor sentence in nltk.corpus.brown.sents():\n    # remove punctuation.\n    # at this point you could also remove stopwords\n    # or iterate over sents_tagged() instead of sents()\n    # to get parts of speech, and only retain\n    # content words\n    wordlist = [w for w in sentence if w.strip(string.punctuation) != \"\"]\n    \n    for targetindex, target in enumerate(wordlist):\n        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n                brown_context_counts[target][contextword] += 1   \n","execution_count":34,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"For this larger corpus, it now makes sense to look at some context word counts to get a sense of what the tables of counts tell us. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 most frequent context words: similar across many items\n# (what can we do about that?)\nprint(\"10 most frequent contexts for some targets:\\n\")\nprint(\"election:\\n\", brown_context_counts[\"election\"].most_common(10))\nprint(\"love:\\n\", brown_context_counts[\"love\"].most_common(10))\nprint(\"car:\", brown_context_counts[\"car\"].most_common(10))","execution_count":35,"outputs":[{"output_type":"stream","text":"10 most frequent contexts for some targets:\n\nelection:\n [('the', 21), ('of', 11), ('to', 6), ('for', 4), ('was', 3), ('and', 3), ('his', 3), ('I', 3), ('on', 3), ('in', 3)]\nlove:\n [('of', 36), ('and', 34), ('in', 26), ('for', 22), ('to', 21), ('the', 17), ('with', 14), ('I', 13), ('you', 10), ('is', 10)]\ncar: [('the', 84), ('a', 36), ('and', 19), ('his', 18), ('was', 13), ('with', 13), ('The', 10), ('is', 8), ('police', 7), ('that', 6)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 100 most frequent context words: now we are starting to see differences.\n# We also see that many of the 100 most frequent context words only have counts of one.\nprint(\"100 most frequent contexts for some targets:\\n\")\nprint(\"election:\\n\", brown_context_counts[\"election\"].most_common(100))\nprint(\"love:\\n\", brown_context_counts[\"love\"].most_common(100))\nprint(\"car:\\n\", brown_context_counts[\"car\"].most_common(100))","execution_count":36,"outputs":[{"output_type":"stream","text":"100 most frequent contexts for some targets:\n\nelection:\n [('the', 21), ('of', 11), ('to', 6), ('for', 4), ('was', 3), ('and', 3), ('his', 3), ('I', 3), ('on', 3), ('in', 3), ('an', 3), ('primary', 2), ('campaign', 2), ('last', 2), ('Presidential', 2), ('results', 2), ('November', 2), ('is', 2), ('April', 2), ('year', 2), ('produced', 1), ('laws', 1), ('general', 1), ('orderly', 1), ('8', 1), ('were', 1), ('investigation', 1), ('day', 1), ('told', 1), ('possible', 1), ('special', 1), ('might', 1), ('did', 1), (\"you'll\", 1), ('received', 1), ('they', 1), ('into', 1), ('bond', 1), ('The', 1), ('will', 1), ('Board', 1), ('Thursday', 1), ('His', 1), ('national', 1), ('close', 1), ('over', 1), ('procedures', 1), ('that', 1), ('inspired', 1), ('missed', 1), ('as', 1), ('falls', 1), ('law', 1), ('dates', 1), ('what', 1), ('plans', 1), ('may', 1)]\nlove:\n [('of', 36), ('and', 34), ('in', 26), ('for', 22), ('to', 21), ('the', 17), ('with', 14), ('I', 13), ('you', 10), ('is', 10), ('that', 7), ('my', 6), ('it', 5), ('we', 5), ('as', 4), ('her', 4), ('this', 4), ('a', 4), ('him', 4), ('us', 3), ('not', 3), ('his', 3), ('but', 3), ('His', 3), ('only', 3), ('true', 3), (\"God's\", 2), ('through', 2), ('without', 2), ('Christian', 2), ('just', 2), ('them', 2), ('was', 2), ('songs', 2), (\"I'd\", 2), ('by', 2), ('romantic', 2), ('My', 2), ('made', 2), ('could', 2), ('can', 1), ('Eisenhower', 1), ('boys', 1), ('children', 1), ('somebody', 1), ('little', 1), (\"mother's\", 1), ('first', 1), ('light', 1), ('deep', 1), ('wisdom', 1), ('common', 1), ('God', 1), ('men', 1), ('earlier', 1), ('people', 1), ('They', 1), ('mutual', 1), ('who', 1), ('great', 1), ('he', 1), ('shade', 1), ('poems', 1), ('another', 1), ('lies', 1), ('force', 1), ('inspired', 1), ('If', 1), ('reflects', 1), ('then', 1), ('on', 1), ('has', 1), ('are', 1), ('impossible', 1), ('nor', 1), ('reality', 1), ('This', 1), ('belongs', 1), ('whose', 1), ('lost', 1), ('more', 1), ('memory', 1), ('letters', 1), ('about', 1), ('knowledge', 1), ('discovered', 1), ('story', 1), ('which', 1), ('song', 1), (\"didn't\", 1), ('they', 1), ('Lord', 1), ('all', 1), ('among', 1), ('way', 1), ('honor', 1), ('she', 1), ('bringing', 1), ('faith', 1), ('would', 1)]\ncar:\n [('the', 84), ('a', 36), ('and', 19), ('his', 18), ('was', 13), ('with', 13), ('The', 10), ('is', 8), ('police', 7), ('that', 6), ('in', 6), ('big', 5), ('motor', 5), ('your', 5), ('parked', 5), ('my', 5), ('to', 4), ('he', 4), ('driven', 4), ('little', 4), ('of', 4), ('second', 4), ('had', 4), ('other', 3), ('new', 3), ('any', 3), ('her', 3), ('but', 3), ('by', 3), ('sports', 3), ('as', 3), ('could', 3), ('will', 3), ('for', 3), ('coming', 3), ('I', 3), ('on', 3), ('passing', 2), ('sales', 2), ('itself', 2), ('which', 2), ('speed', 2), ('box', 2), ('another', 2), ('at', 2), ('before', 2), ('industry', 2), ('until', 2), ('freight', 2), ('she', 2), ('into', 2), ('agency', 2), ('their', 2), ('door', 2), ('approaching', 2), ('here', 2), ('patrol', 2), ('own', 2), ('it', 2), ('old', 2), ('Street', 1), ('apart', 1), ('flying', 1), ('family', 1), ('throw', 1), (\"mother's\", 1), ('wanted', 1), ('heading', 1), ('number', 1), ('when', 1), ('they', 1), ('evidently', 1), ('used', 1), ('brushed', 1), ('New', 1), ('wrong', 1), ('now', 1), ('one', 1), ('herself', 1), (\"they'd\", 1), (\"isn't\", 1), ('foreign', 1), ('you', 1), ('coat', 1), ('My', 1), ('like', 1), ('Army', 1), ('smaller', 1), ('free', 1), ('back', 1), ('private', 1), ('driving', 1), ('must', 1), ('determine', 1), ('much', 1), ('buying', 1), ('covering', 1), ('more', 1), ('model', 1), ('might', 1)]\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# some ambiguous words\nprint(\"Some ambiguous words:\")\nprint(\"bat:\\n\", brown_context_counts[\"bat\"].most_common(100))\nprint(\"bank:\\n\", brown_context_counts[\"bank\"].most_common(100))\nprint(\"bar:\\n\", brown_context_counts[\"bar\"].most_common(100))\nprint(\"leave:\\n\", brown_context_counts[\"leave\"].most_common(100))","execution_count":37,"outputs":[{"output_type":"stream","text":"Some ambiguous words:\nbat:\n []\nbank:\n [('the', 23), ('of', 9), ('a', 4), ('and', 4), ('in', 3), ('The', 2), ('local', 2), ('south', 2), ('which', 2), ('east', 2), ('west', 2), ('over', 1), ('accounts', 1), ('customers', 1), ('have', 1), ('That', 1), ('installed', 1), ('said', 1), ('is', 1), ('policy', 1), ('cloud', 1), ('that', 1), ('would', 1), ('for', 1), ('handling', 1), ('president', 1), ('to', 1), ('with', 1), ('officials', 1), ('before', 1), ('by', 1), ('loans', 1), ('left', 1), ('wrong', 1), ('far', 1), ('soft', 1), ('toward', 1), ('through', 1), ('river', 1), ('high', 1), ('outside', 1), ('big', 1), ('roll', 1)]\nbar:\n [('the', 34), ('and', 10), ('locking', 10), ('a', 4), ('Af', 3), ('in', 3), ('while', 2), ('he', 2), ('to', 2), ('patent', 2), ('Would', 1), ('vehicles', 1), ('without', 1), ('cocktail', 1), ('which', 1), ('come', 1), ('at', 1), ('held', 1), ('A', 1), ('stock', 1), ('on', 1), ('should', 1), ('our', 1), ('is', 1), ('as', 1), ('who', 1), ('with', 1), ('so', 1), ('absolute', 1), ('not', 1), ('finding', 1), ('little', 1), ('next', 1), ('was', 1), ('breakfast', 1), ('kitchen', 1), ('off', 1), ('headed', 1), ('same', 1), ('some', 1), ('if', 1), ('windows', 1), ('running', 1), ('but', 1), ('top', 1), ('crowded', 1)]\nleave:\n [('to', 79), ('the', 41), ('and', 17), ('it', 14), ('you', 10), ('him', 9), ('I', 8), ('not', 8), ('her', 8), ('his', 6), ('a', 6), ('this', 5), ('they', 4), ('my', 4), ('for', 4), ('France', 4), ('me', 4), ('here', 3), ('will', 3), ('with', 3), ('of', 3), ('but', 3), ('on', 3), ('their', 3), ('or', 3), ('just', 3), (\"can't\", 3), ('behind', 2), ('we', 2), ('us', 2), ('them', 2), ('because', 2), ('in', 2), ('at', 2), (\"couldn't\", 2), ('cannot', 2), ('from', 2), ('better', 2), ('never', 2), ('before', 2), ('all', 2), ('that', 2), (\"wouldn't\", 2), (\"he'd\", 2), ('could', 2), (\"I'll\", 2), ('one', 2), ('Eisenhower', 1), ('Capitol', 1), ('Sunday', 1), ('have', 1), ('both', 1), ('can', 1), ('until', 1), ('Parker', 1), ('clay', 1), ('seldom', 1), ('no', 1), ('some', 1), ('received', 1), ('he', 1), ('simply', 1), ('earlier', 1), ('through', 1), ('people', 1), ('out', 1), ('ever', 1), ('room', 1), ('replied', 1), ('Mr.', 1), ('may', 1), ('London', 1), ('therefore', 1), ('our', 1), ('everything', 1), ('Rome', 1), ('General', 1), ('Under', 1), ('administrative', 1), ('then', 1), ('which', 1), ('is', 1), ('dominant', 1), ('trying', 1), ('also', 1), ('1', 1), ('should', 1), ('New', 1), ('take', 1), ('must', 1), ('off', 1), ('only', 1), ('need', 1), ('would', 1), ('But', 1), ('around', 1), ('your', 1), (\"don't\", 1), ('than', 1), (\"'em\", 1)]\n","name":"stdout"}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Working with numpy matrices\n\nWe re-compute the whole space as a matrix, this time using numpy, because this corpus is already too big to fit comfortably into pandas. (Meaning, it takes forever to compute the dataframes.) "},{"metadata":{"trusted":true},"cell_type":"code","source":"# first determine the list of all words in Brown\n# repeat: frequency threshold\nfrequency_threshold = 20\n\nbrown_wordlist = list(w for w in brown_wordcounts if brown_wordcounts[w] >= frequency_threshold)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a dictionary that maps each word to its index in the wordlist\nbrown_wordlist_lookup = dict((word, index) for index, word in enumerate(brown_wordlist))","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n# We need an array with enough space to hold \n# len(brown_wordlist) target words, and\n# len(brown_wordlist) context words.\n# We first initialize it to all zeros.\nbrown_count_matrix = np.zeros((len(brown_wordlist), len(brown_wordlist)))","execution_count":40,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# Now, let's do the context word counting with this matrix.\n\nimport string\n\nfor sentence in nltk.corpus.brown.sents():\n    # remove punctuation.\n    # at this point you could also remove stopwords\n    # or iterate over sents_tagged() instead of sents()\n    # to get parts of speech, and only retain\n    # content words\n    wordlist = [w for w in sentence if w.strip(string.punctuation) != \"\"]\n    \n    for targetindex, target in enumerate(wordlist):\n        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n                # which cell in the matrix is this? \n                # look up both the target and the context word\n                # in the ordered list of Brown words\n                targetindex_matrix = brown_wordlist_lookup[target]\n                contextindex_matrix = brown_wordlist_lookup[contextword]\n                # and add a count of one for this cell in the matrix\n                brown_count_matrix[targetindex_matrix][contextindex_matrix] += 1   \n","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can again compute similarity in this space\nsaid_index = brown_wordlist_lookup[\"said\"]\nwrote_index = brown_wordlist_lookup[\"wrote\"]\ncosine_sim( brown_count_matrix[said_index], brown_count_matrix[wrote_index])","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"0.8750717787803817"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"We now compute PMI again, making use of the fact that\n\n$PMI(a, b) = \\log \\frac{P(a, b)}{P(a)P(b)} = \\log\\frac{P(b|a)}{P(b)}$\n\nnumpy offers functions that apply an operation to a whole vector at once,\nrather than one at a time. \nThis is much quicker."},{"metadata":{"trusted":true},"cell_type":"code","source":"# And we can compute PMI again.\n# numpy offers functions that\n# apply an operation to a whole vector at once,\n# rather than one at a time. \n# This is much quicker.\n\ncount_all = brown_count_matrix.sum()\n\n# probability of contexts/columns:\n# this is our P(b)\n# This is a vector with a probability for each context\n# sum(axis=0) is numpy's way of saying that we want to sum each column\ncol_totals = brown_count_matrix.sum(axis=0)\n# avoid zeros, they get us in trouble later when we divide by p_c\ncol_totals[col_totals == 0] = 0.00001\n# this is a vector where each row total is divided by the overall count\np_c = col_totals / count_all\n\n# probability of context given target:\n# this is our P(b|a)\n# sum(axis=1) is numpy's way of saying that we want to sum each row:\n# we divide each row by its row total, getting the probability of a context item\n# within this target. \n# do do this, we flip the matrix on its side so that columns become rows,\n# then do the division (otherwise numpy would do column-wise instead of row-wise division)\nrow_totals = brown_count_matrix.sum(axis=1).astype(float)\nrow_totals[row_totals == 0] = 0.00001\np_c_given_t = (brown_count_matrix.T / row_totals).T\n\n# PMI: log( P(b|a) / P(b))\n# we again divide a matrix by a vector\n# this time we do want column-wise division\n# so we don't have to flip the matrix\npct_divided_by_pc = p_c_given_t / p_c\n# avoid doing log(0)by replacing 0 by a small number\npct_divided_by_pc[pct_divided_by_pc==0] = 0.00001\n\nbrown_pmi_matrix = np.log(pct_divided_by_pc)\n","execution_count":43,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"# and computing similarity again\nsaid_index = brown_wordlist_lookup[\"said\"]\nwrote_index = brown_wordlist_lookup[\"wrote\"]\ncosine_sim( brown_pmi_matrix[said_index], brown_pmi_matrix[wrote_index])","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"0.9244136837103797"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nearest_neighbors_obj = NearestNeighbors(n_neighbors=20, metric = scipy.spatial.distance.cosine)\n\n# we then allow it to compute an internal datastructure from our data\nnearest_neighbors_obj.fit(brown_pmi_matrix)","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"NearestNeighbors(metric=<function cosine at 0x7f3439f60550>, n_neighbors=20)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([brown_pmi_matrix[said_index]])","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saidNN = pd.DataFrame(\n    np.c_[\n        np.array([brown_wordlist[i] for i in [target_indices][0][0]]), \n        cosine_distances[0]\n    ],\n    columns = ['word', 'cosine similarity']\n)\n","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"saidNN","execution_count":48,"outputs":[{"output_type":"execute_result","execution_count":48,"data":{"text/plain":"         word    cosine similarity\n0        said                  0.0\n1        told  0.06925978938395227\n2   suspected  0.07028412566201891\n3      smiled  0.07041002420236053\n4      talked  0.07048890962242338\n5    remarked  0.07054217639994487\n6   impressed  0.07065588960128677\n7    happened  0.07068130870191813\n8   forgotten  0.07078041856535933\n9     crossed  0.07079818625325662\n10    laughed  0.07092304173412167\n11   wondered  0.07093142453288204\n12   remember  0.07095636450053655\n13      asked  0.07102160672561997\n14      fired  0.07110430028519388\n15    grabbed  0.07113687651011669\n16     wonder  0.07117718106960946\n17   regarded  0.07117866695332009\n18   released  0.07117960861561234\n19    noticed  0.07119119891395131","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>cosine similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>said</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>told</td>\n      <td>0.06925978938395227</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>suspected</td>\n      <td>0.07028412566201891</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>smiled</td>\n      <td>0.07041002420236053</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>talked</td>\n      <td>0.07048890962242338</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>remarked</td>\n      <td>0.07054217639994487</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>impressed</td>\n      <td>0.07065588960128677</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>happened</td>\n      <td>0.07068130870191813</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>forgotten</td>\n      <td>0.07078041856535933</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>crossed</td>\n      <td>0.07079818625325662</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>laughed</td>\n      <td>0.07092304173412167</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>wondered</td>\n      <td>0.07093142453288204</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>remember</td>\n      <td>0.07095636450053655</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>asked</td>\n      <td>0.07102160672561997</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>fired</td>\n      <td>0.07110430028519388</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>grabbed</td>\n      <td>0.07113687651011669</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>wonder</td>\n      <td>0.07117718106960946</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>regarded</td>\n      <td>0.07117866695332009</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>released</td>\n      <td>0.07117960861561234</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>noticed</td>\n      <td>0.07119119891395131</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"## Creating a term-document matrix\n\nData are taken from https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","source":"import os\ntry:\n    os.makedirs('../data')\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://unitc-my.sharepoint.com/:x:/g/personal/nwsja01_cloud_uni-tuebingen_de/ESJ8QyqzmxpInK_jMQvQ_LEB9XoRbcw39rFmM6ZAQjrwvA?download=1 -O ../data/Reviews.csv","execution_count":50,"outputs":[{"output_type":"stream","text":"--2022-05-01 11:20:35--  https://unitc-my.sharepoint.com/:x:/g/personal/nwsja01_cloud_uni-tuebingen_de/ESJ8QyqzmxpInK_jMQvQ_LEB9XoRbcw39rFmM6ZAQjrwvA?download=1\nResolving unitc-my.sharepoint.com (unitc-my.sharepoint.com)... 40.108.241.27\nConnecting to unitc-my.sharepoint.com (unitc-my.sharepoint.com)|40.108.241.27|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /personal/nwsja01_cloud_uni-tuebingen_de/Documents/vectorSpaceSemantics2022/data/Reviews.csv?ga=1 [following]\n--2022-05-01 11:20:35--  https://unitc-my.sharepoint.com/personal/nwsja01_cloud_uni-tuebingen_de/Documents/vectorSpaceSemantics2022/data/Reviews.csv?ga=1\nReusing existing connection to unitc-my.sharepoint.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 300904694 (287M) [application/octet-stream]\nSaving to: â../data/Reviews.csvâ\n\n../data/Reviews.csv 100%[===================>] 286.96M  28.1MB/s    in 10s     \n\n2022-05-01 11:20:46 (28.4 MB/s) - â../data/Reviews.csvâ saved [300904694/300904694]\n\n","name":"stdout"}]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../data/Reviews.csv\")\ndf","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"            Id   ProductId          UserId                      ProfileName  \\\n0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n...        ...         ...             ...                              ...   \n568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n\n        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n0                          1                       1      5  1303862400   \n1                          0                       0      1  1346976000   \n2                          1                       1      4  1219017600   \n3                          3                       3      2  1307923200   \n4                          0                       0      5  1350777600   \n...                      ...                     ...    ...         ...   \n568449                     0                       0      5  1299628800   \n568450                     0                       0      2  1331251200   \n568451                     2                       2      5  1329782400   \n568452                     1                       1      5  1331596800   \n568453                     0                       0      5  1338422400   \n\n                                   Summary  \\\n0                    Good Quality Dog Food   \n1                        Not as Advertised   \n2                    \"Delight\" says it all   \n3                           Cough Medicine   \n4                              Great taffy   \n...                                    ...   \n568449                 Will not do without   \n568450                        disappointed   \n568451            Perfect for our maltipoo   \n568452  Favorite Training and reward treat   \n568453                         Great Honey   \n\n                                                     Text  \n0       I have bought several of the Vitality canned d...  \n1       Product arrived labeled as Jumbo Salted Peanut...  \n2       This is a confection that has been around a fe...  \n3       If you are looking for the secret ingredient i...  \n4       Great taffy at a great price.  There was a wid...  \n...                                                   ...  \n568449  Great for sesame chicken..this is a good if no...  \n568450  I'm disappointed with the flavor. The chocolat...  \n568451  These stars are small, so you can give 10-15 o...  \n568452  These are the BEST treats for training and rew...  \n568453  I am very satisfied ,product is as advertised,...  \n\n[568454 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>Natalia Corres \"Natalia Corres\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1219017600</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>Karl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1307923200</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>Michael D. Bigham \"M. Wassir\"</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1350777600</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>568449</th>\n      <td>568450</td>\n      <td>B001EO7N10</td>\n      <td>A28KG5XORO54AY</td>\n      <td>Lettie D. Carter</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1299628800</td>\n      <td>Will not do without</td>\n      <td>Great for sesame chicken..this is a good if no...</td>\n    </tr>\n    <tr>\n      <th>568450</th>\n      <td>568451</td>\n      <td>B003S1WTCU</td>\n      <td>A3I8AFVPEE8KI5</td>\n      <td>R. Sawyer</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1331251200</td>\n      <td>disappointed</td>\n      <td>I'm disappointed with the flavor. The chocolat...</td>\n    </tr>\n    <tr>\n      <th>568451</th>\n      <td>568452</td>\n      <td>B004I613EE</td>\n      <td>A121AA1GQV751Z</td>\n      <td>pksd \"pk_007\"</td>\n      <td>2</td>\n      <td>2</td>\n      <td>5</td>\n      <td>1329782400</td>\n      <td>Perfect for our maltipoo</td>\n      <td>These stars are small, so you can give 10-15 o...</td>\n    </tr>\n    <tr>\n      <th>568452</th>\n      <td>568453</td>\n      <td>B004I613EE</td>\n      <td>A3IBEVCTXKNOH</td>\n      <td>Kathy A. Welch \"katwel\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1331596800</td>\n      <td>Favorite Training and reward treat</td>\n      <td>These are the BEST treats for training and rew...</td>\n    </tr>\n    <tr>\n      <th>568453</th>\n      <td>568454</td>\n      <td>B001LR2CU2</td>\n      <td>A3LGQPJCZVL9UC</td>\n      <td>srfell17</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1338422400</td>\n      <td>Great Honey</td>\n      <td>I am very satisfied ,product is as advertised,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>568454 rows Ã 10 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"np.shape(df)","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"(568454, 10)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"name":"python3","display_name":"Python 3 (ipykernel)","language":"python"},"language_info":{"name":"python","version":"3.9.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":2}