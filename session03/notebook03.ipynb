{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector space semantics\n",
    "\n",
    "## Session 03: Dimensionality reduction\n",
    "\n",
    "### Gerhard JÃ¤ger\n",
    "\n",
    "\n",
    "May 9, 2022\n",
    "\n",
    "(partially based on slides by Katrin Erk, https://www.katrinerk.com/courses/lin350-computational-semantics, with kind permission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using a somewhat larger corpus\n",
    "\n",
    "We next demonstrate a somewhat larger corpus, with yet another method of accessing the corpus data: If the data is available within the NLTK corpora, you can use the NLTK's corpus reader to access it.\n",
    "\n",
    "The Brown corpus is a 1 million word corpus of carefully selected text pieces from different genres, originally made to support dictionary-makers, so it's intended to cover a broad variety of genres in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few sentences of the Brown corpus:\n",
      "\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'] \n",
      "\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'] \n",
      "\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.'] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/gjaeger/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "print(\"The first few sentences of the Brown corpus:\\n\")\n",
    "for s in nltk.corpus.brown.sents()[:3]: \n",
    "    print(s, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We compute the target/context counts, noting context items as we go. We only count words that appear at least 10 times in the corpus. This cuts down a lot on the size of our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011),\n",
       " ('was', 9777),\n",
       " ('for', 8841),\n",
       " ('``', 8837),\n",
       " (\"''\", 8789),\n",
       " ('The', 7258),\n",
       " ('with', 7012),\n",
       " ('it', 6723),\n",
       " ('as', 6706),\n",
       " ('he', 6566),\n",
       " ('his', 6466)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_wordcounts = nltk.FreqDist(nltk.corpus.brown.words())\n",
    "brown_wordcounts.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## remove punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gjaeger/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all',\n",
       "       'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at',\n",
       "       'be', 'because', 'been', 'before', 'being', 'below', 'between',\n",
       "       'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did',\n",
       "       'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don',\n",
       "       \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further',\n",
       "       'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven',\n",
       "       \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him',\n",
       "       'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn',\n",
       "       \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
       "       'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\",\n",
       "       'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o',\n",
       "       'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours',\n",
       "       'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan',\n",
       "       \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn',\n",
       "       \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\",\n",
       "       'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there',\n",
       "       'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under',\n",
       "       'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were',\n",
       "       'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while',\n",
       "       'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn',\n",
       "       \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
       "       'your', 'yours', 'yourself', 'yourselves'], dtype='<U10')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = np.unique(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wordlist(wl):\n",
    "    wl = [w.lower().strip(string.punctuation) for w in wl]\n",
    "    return [w for w in wl if w != \"\" and not w in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fulton',\n",
       " 'county',\n",
       " 'grand',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'friday',\n",
       " 'investigation',\n",
       " \"atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " 'evidence',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'term-end',\n",
       " 'presentments',\n",
       " 'city',\n",
       " 'executive',\n",
       " 'committee',\n",
       " 'over-all',\n",
       " 'charge',\n",
       " 'election',\n",
       " 'deserves',\n",
       " 'praise',\n",
       " 'thanks',\n",
       " 'city',\n",
       " 'atlanta',\n",
       " 'manner',\n",
       " 'election',\n",
       " 'conducted',\n",
       " 'september-october',\n",
       " 'term',\n",
       " 'jury',\n",
       " 'charged',\n",
       " 'fulton',\n",
       " 'superior']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_words = normalize_wordlist(nltk.corpus.brown.words())\n",
    "brown_words[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 3297),\n",
       " ('would', 2714),\n",
       " ('said', 1961),\n",
       " ('new', 1635),\n",
       " ('could', 1601),\n",
       " ('time', 1598),\n",
       " ('two', 1412),\n",
       " ('may', 1402),\n",
       " ('first', 1361),\n",
       " ('like', 1292),\n",
       " ('man', 1207),\n",
       " ('even', 1170),\n",
       " ('made', 1125),\n",
       " ('also', 1069),\n",
       " ('many', 1030),\n",
       " ('must', 1013),\n",
       " ('af', 996),\n",
       " ('back', 966),\n",
       " ('years', 958),\n",
       " ('much', 937),\n",
       " ('way', 909),\n",
       " ('well', 897),\n",
       " ('people', 847),\n",
       " ('mr', 844),\n",
       " ('little', 831),\n",
       " ('state', 807),\n",
       " ('good', 806),\n",
       " ('make', 794),\n",
       " ('world', 787),\n",
       " ('still', 782),\n",
       " ('see', 772),\n",
       " ('men', 763),\n",
       " ('work', 762),\n",
       " ('long', 753),\n",
       " ('get', 749),\n",
       " ('life', 715),\n",
       " ('never', 697),\n",
       " ('day', 687),\n",
       " ('another', 684),\n",
       " ('know', 683)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_wordcounts = nltk.FreqDist(brown_words)\n",
    "brown_wordcounts.most_common(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fulton',\n",
       "  'county',\n",
       "  'grand',\n",
       "  'jury',\n",
       "  'said',\n",
       "  'friday',\n",
       "  'investigation',\n",
       "  \"atlanta's\",\n",
       "  'recent',\n",
       "  'primary',\n",
       "  'election',\n",
       "  'produced',\n",
       "  'evidence',\n",
       "  'irregularities',\n",
       "  'took',\n",
       "  'place'],\n",
       " ['jury',\n",
       "  'said',\n",
       "  'term-end',\n",
       "  'presentments',\n",
       "  'city',\n",
       "  'executive',\n",
       "  'committee',\n",
       "  'over-all',\n",
       "  'charge',\n",
       "  'election',\n",
       "  'deserves',\n",
       "  'praise',\n",
       "  'thanks',\n",
       "  'city',\n",
       "  'atlanta',\n",
       "  'manner',\n",
       "  'election',\n",
       "  'conducted'],\n",
       " ['september-october',\n",
       "  'term',\n",
       "  'jury',\n",
       "  'charged',\n",
       "  'fulton',\n",
       "  'superior',\n",
       "  'court',\n",
       "  'judge',\n",
       "  'durwood',\n",
       "  'pye',\n",
       "  'investigate',\n",
       "  'reports',\n",
       "  'possible',\n",
       "  'irregularities',\n",
       "  'hard-fought',\n",
       "  'primary',\n",
       "  'mayor-nominate',\n",
       "  'ivan',\n",
       "  'allen',\n",
       "  'jr'],\n",
       " ['relative',\n",
       "  'handful',\n",
       "  'reports',\n",
       "  'received',\n",
       "  'jury',\n",
       "  'said',\n",
       "  'considering',\n",
       "  'widespread',\n",
       "  'interest',\n",
       "  'election',\n",
       "  'number',\n",
       "  'voters',\n",
       "  'size',\n",
       "  'city']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_sents = [normalize_wordlist(s) for s in nltk.corpus.brown.sents()]\n",
    "brown_sents = [s for s in brown_sents if len(s) > 0]\n",
    "brown_sents[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56831, 537755, 49039)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(brown_sents), sum([len(s) for s in brown_sents]), len(np.unique(brown_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_contextword_1wordwindow(wordlist, targetindex):\n",
    "    if targetindex > 0:\n",
    "        # preceding word\n",
    "        yield wordlist[targetindex - 1]\n",
    "        \n",
    "    if targetindex < len(wordlist)- 1:\n",
    "        # succeeding word\n",
    "        yield wordlist[targetindex + 1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "brown_context_counts = nltk.ConditionalFreqDist()\n",
    "\n",
    "frequency_threshold = 20\n",
    "\n",
    "for sentence in brown_sents:\n",
    "    \n",
    "    for targetindex, target in enumerate(sentence):\n",
    "        for contextword in each_contextword_1wordwindow(sentence, targetindex):\n",
    "            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n",
    "                brown_context_counts[target][contextword] += 1   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this larger corpus, it now makes sense to look at some context word counts to get a sense of what the tables of counts tell us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent contexts for some targets:\n",
      "\n",
      "election:\n",
      " [('day', 5), ('presidential', 5), ('results', 4), ('board', 3), ('primary', 2), ('interest', 2), ('ever', 2), ('campaign', 2), ('judges', 2), ('last', 2)]\n",
      "love:\n",
      " [('god', 8), ('way', 6), ('us', 5), ('know', 4), ('faith', 4), ('country', 4), ('true', 4), ('give', 3), (\"mother's\", 3), ('fell', 3)]\n",
      "car: [('police', 7), ('got', 7), ('parked', 7), ('big', 6), ('drive', 6), ('approaching', 5), ('driving', 5), ('driven', 5), ('motor', 5), ('coming', 5)]\n"
     ]
    }
   ],
   "source": [
    "# 10 most frequent context words: similar across many items\n",
    "# (what can we do about that?)\n",
    "print(\"10 most frequent contexts for some targets:\\n\")\n",
    "print(\"election:\\n\", brown_context_counts[\"election\"].most_common(10))\n",
    "print(\"love:\\n\", brown_context_counts[\"love\"].most_common(10))\n",
    "print(\"car:\", brown_context_counts[\"car\"].most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most frequent contexts for some targets:\n",
      "\n",
      "election:\n",
      " [('day', 5), ('presidential', 5), ('results', 4), ('board', 3), ('primary', 2), ('interest', 2), ('ever', 2), ('campaign', 2), ('judges', 2), ('last', 2), ('november', 2), ('since', 2), ('close', 2), ('april', 2), ('year', 2), ('produced', 1), ('charge', 1), ('manner', 1), ('conducted', 1), ('number', 1), ('registration', 1), ('laws', 1), ('back', 1), ('city', 1), ('general', 1), ('leading', 1), ('quiet', 1), ('scheduled', 1), ('orderly', 1), ('8', 1), ('involved', 1), ('investigation', 1), ('attorney', 1), ('told', 1), ('possible', 1), ('nine', 1), ('special', 1), ('might', 1), ('mean', 1), ('program', 1), ('think', 1), ('received', 1), ('president', 1), ('right', 1), ('almost', 1), ('pull', 1), ('bond', 1), ('construction', 1), ('came', 1), ('thursday', 1), ('set', 1), ('pick', 1), ('howard', 1), ('declaration', 1), ('man', 1), ('result', 1), ('hand', 1), ('national', 1), ('left', 1), ('whether', 1), ('60', 1), ('religious', 1), ('boards', 1), ('properly', 1), ('procedures', 1), ('lost', 1), ('american', 1), ('deal', 1), ('doctrine', 1), ('inspired', 1), ('including', 1), ('academy', 1), ('though', 1), ('interpreted', 1), ('suppose', 1), ('september', 1), ('missed', 1), ('governor', 1), ('quiney', 1), ('analysis', 1), ('falls', 1), ('aspect', 1), ('actual', 1), ('announced', 1), ('law', 1), ('dates', 1), ('doubt', 1), ('plans', 1), ('may', 1)]\n",
      "love:\n",
      " [('god', 8), ('way', 6), ('us', 5), ('know', 4), ('faith', 4), ('country', 4), ('true', 4), ('give', 3), (\"mother's\", 3), ('fell', 3), ('man', 3), ('life', 3), ('knowledge', 3), ('fallen', 3), ('hate', 3), ('johnnie', 3), ('bound', 2), (\"god's\", 2), ('little', 2), ('without', 2), ('like', 2), ('fall', 2), ('pretty', 2), ('even', 2), ('christ', 2), ('christian', 2), ('another', 2), ('expressed', 2), ('three', 2), ('songs', 2), ('nature', 2), (\"i'd\", 2), ('yet', 2), ('world', 2), ('force', 2), ('almost', 2), ('mother', 2), ('returned', 2), ('lost', 2), ('romantic', 2), ('law', 2), ('marriage', 2), ('capacity', 2), ('well', 2), ('suffer', 2), ('made', 2), ('poem', 2), ('real', 2), ('could', 2), ('added', 1), ('eisenhower', 1), ('boys', 1), ('children', 1), ('somebody', 1), ('state', 1), ('usual', 1), ('interesting', 1), ('message', 1), ('comfort', 1), ('expects', 1), ('responsible', 1), ('friends', 1), ('service', 1), ('first', 1), ('last', 1), ('light', 1), ('singing', 1), ('experience', 1), ('born', 1), ('deep', 1), ('remained', 1), ('gone', 1), ('jazz', 1), ('spirit', 1), ('wisdom', 1), ('common', 1), ('understanding', 1), ('men', 1), ('strength', 1), ('said', 1), ('course', 1), ('dances', 1), ('earlier', 1), ('literature', 1), ('run', 1), ('people', 1), ('crack', 1), ('dust', 1), ('takes', 1), ('mutual', 1), ('best', 1), ('respects', 1), ('folklore', 1), ('college', 1), ('great', 1), ('flowers', 1), ('sex', 1), ('sexual', 1), ('except', 1), ('seems', 1)]\n",
      "car:\n",
      " [('police', 7), ('got', 7), ('parked', 7), ('big', 6), ('drive', 6), ('approaching', 5), ('driving', 5), ('driven', 5), ('motor', 5), ('coming', 5), ('take', 4), ('road', 4), ('new', 4), ('get', 4), ('little', 4), ('one', 4), ('second', 4), ('work', 3), ('patrol', 3), ('drove', 3), ('sports', 3), ('back', 3), ('left', 3), ('could', 3), ('speed', 3), ('another', 3), ('reserve', 3), ('waiting', 3), ('side', 3), ('see', 3), ('waited', 3), ('sight', 3), ('knew', 3), ('wanted', 2), ('three', 2), ('said', 2), ('number', 2), ('passing', 2), ('riding', 2), ('brushed', 2), ('sales', 2), ('wife', 2), ('must', 2), ('says', 2), ('point', 2), ('fast', 2), ('stayed', 2), ('minutes', 2), ('box', 2), ('provided', 2), ('europe', 2), ('industry', 2), ('freight', 2), ('truck', 2), ('next', 2), ('agency', 2), ('license', 2), ('still', 2), ('seat', 2), ('door', 2), ('sit', 2), ('around', 2), ('pulled', 2), ('ran', 2), ('came', 2), ('old', 2), ('street', 1), ('apart', 1), ('flying', 1), ('family', 1), ('throw', 1), (\"mother's\", 1), ('description', 1), ('immediately', 1), ('afternoon', 1), ('heading', 1), ('gets', 1), ('suddenly', 1), ('friday', 1), ('evidently', 1), ('taken', 1), ('used', 1), ('accident', 1), ('struck', 1), ('suffered', 1), ('block', 1), ('school', 1), ('wrong', 1), (\"they'd\", 1), ('capable', 1), ('foreign', 1), ('wear', 1), ('coat', 1), ('tail', 1), ('kind', 1), ('like', 1), ('army', 1), ('smaller', 1), ('moment', 1), ('free', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 100 most frequent context words\n",
    "# We see that many of the 100 most frequent context words only have counts of one.\n",
    "print(\"100 most frequent contexts for some targets:\\n\")\n",
    "print(\"election:\\n\", brown_context_counts[\"election\"].most_common(100))\n",
    "print(\"love:\\n\", brown_context_counts[\"love\"].most_common(100))\n",
    "print(\"car:\\n\", brown_context_counts[\"car\"].most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some ambiguous words:\n",
      "bank:\n",
      " [('washington', 7), ('national', 3), ('prepared', 3), ('west', 3), ('red', 2), ('french', 2), ('new', 2), ('local', 2), ('south', 2), ('held', 2), ('east', 2), ('manchester', 2), ('edge', 2), ('river', 2), ('take', 1), ('accounts', 1), ('confidence', 1), ('customers', 1), ('also', 1), ('even', 1), ('savings', 1), ('chicago', 1), ('first', 1), ('jersey', 1), ('installed', 1), ('visited', 1), ('march', 1), ('people', 1), ('said', 1), ('touch', 1), ('reserve', 1), ('done', 1), ('policy', 1), ('make', 1), ('cloud', 1), ('hope', 1), ('representative', 1), ('end', 1), ('would', 1), ('world', 1), ('firm', 1), ('later', 1), ('handling', 1), ('sighed', 1), ('president', 1), ('destroy', 1), ('southern', 1), ('approval', 1), ('contact', 1), ('commerce', 1), ('officials', 1), ('cash', 1), ('may', 1), ('loans', 1), ('whether', 1), ('received', 1), ('manhattan', 1), ('providence', 1), ('left', 1), ('north', 1), ('put', 1), ('green', 1), ('man', 1), ('brought', 1), ('coming', 1), ('hudson', 1), ('waiting', 1), ('wrong', 1), ('far', 1), ('soft', 1), ('back', 1), ('toward', 1), ('stumbled', 1), ('passed', 1), ('mrs', 1), ('appointed', 1), ('cap', 1), ('go', 1), ('high', 1), ('outside', 1), ('big', 1), ('roll', 1)]\n",
      "bar:\n",
      " [('locking', 10), ('back', 4), ('end', 4), ('association', 3), ('af', 3), ('running', 2), ('rest', 2), ('top', 2), ('patent', 2), ('midnight', 2), ('atlanta', 1), ('would', 1), ('vehicles', 1), ('chicago', 1), ('american', 1), ('body', 1), ('without', 1), ('singing', 1), ('cocktail', 1), ('feel', 1), ('come', 1), ('hold', 1), ('experiments', 1), ('held', 1), ('12', 1), ('lower', 1), ('stock', 1), ('across', 1), ('remain', 1), ('c', 1), ('proper', 1), ('lines', 1), ('rough', 1), ('sink', 1), ('recommended', 1), ('eating', 1), ('effort', 1), ('introduced', 1), ('mr', 1), ('long', 1), ('admitted', 1), ('career', 1), ('considerable', 1), ('judge', 1), ('day', 1), ('legend', 1), ('temple', 1), ('absolute', 1), ('recovery', 1), ('however', 1), ('finding', 1), ('little', 1), ('next', 1), ('hanging', 1), ('oil', 1), ('breakfast', 1), ('kitchen', 1), ('main', 1), ('beside', 1), ('bill', 1), ('front', 1), ('headed', 1), ('called', 1), ('two', 1), ('b', 1), ('forth', 1), (\"he's\", 1), ('view', 1), ('windows', 1), ('met', 1), ('attempt', 1), ('taking', 1), ('towards', 1), ('probably', 1), ('crowded', 1), ('remember', 1), ('san', 1)]\n",
      "leave:\n",
      " [('us', 5), ('alone', 5), ('france', 5), ('would', 5), ('want', 5), ('take', 4), (\"can't\", 4), ('room', 3), ('morning', 3), ('one', 3), ('country', 3), ('better', 3), ('decided', 3), ('instructions', 3), ('could', 3), (\"i'll\", 3), ('party', 3), ('get', 2), ('behind', 2), ('right', 2), ('going', 2), ('let', 2), ('parker', 2), ('peace', 2), ('clay', 2), ('table', 2), ('people', 2), ('cannot', 2), ('farm', 2), ('never', 2), ('permission', 2), ('alfred', 2), ('extend', 2), ('nest', 2), ('job', 2), (\"he'd\", 2), ('wife', 2), ('time', 2), ('last', 2), ('sure', 2), ('eisenhower', 1), ('capitol', 1), ('coach', 1), ('sunday', 1), ('set', 1), ('church', 1), ('aid', 1), ('confused', 1), ('minute', 1), ('home', 1), ('applications', 1), ('compromise', 1), ('sides', 1), ('heard', 1), ('upon', 1), ('feeling', 1), ('association', 1), ('took', 1), ('learned', 1), ('power', 1), ('world', 1), ('open', 1), ('c', 1), ('mold', 1), ('seldom', 1), ('storm', 1), ('excuse', 1), ('work', 1), ('paper', 1), ('interest', 1), ('company', 1), ('car', 1), ('europe', 1), ('arrival', 1), ('another', 1), ('permit', 1), ('rent', 1), ('service', 1), ('first', 1), ('completely', 1), ('trace', 1), ('asked', 1), ('hotel', 1), ('forced', 1), ('cattle', 1), ('caused', 1), ('returned', 1), ('received', 1), ('effect', 1), ('pure', 1), ('simply', 1), ('negroes', 1), ('request', 1), ('earlier', 1), ('saw', 1), ('side', 1), ('opposite', 1), ('account', 1), ('ever', 1), ('house', 1)]\n"
     ]
    }
   ],
   "source": [
    "# some ambiguous words\n",
    "print(\"Some ambiguous words:\")\n",
    "print(\"bank:\\n\", brown_context_counts[\"bank\"].most_common(100))\n",
    "print(\"bar:\\n\", brown_context_counts[\"bar\"].most_common(100))\n",
    "print(\"leave:\\n\", brown_context_counts[\"leave\"].most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with numpy matrices\n",
    "\n",
    "We re-compute the whole space as a matrix, this time using numpy, because this corpus is already too big to fit comfortably into pandas. (Meaning, it takes forever to compute the dataframes.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first determine the list of all words in Brown\n",
    "# repeat: frequency threshold\n",
    "frequency_threshold = 20\n",
    "\n",
    "brown_wordlist = list(w for w in brown_wordcounts if brown_wordcounts[w] >= frequency_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'would',\n",
       " 'said',\n",
       " 'new',\n",
       " 'could',\n",
       " 'time',\n",
       " 'two',\n",
       " 'may',\n",
       " 'first',\n",
       " 'like',\n",
       " 'man',\n",
       " 'even',\n",
       " 'made',\n",
       " 'also',\n",
       " 'many',\n",
       " 'must',\n",
       " 'af',\n",
       " 'back',\n",
       " 'years',\n",
       " 'much',\n",
       " 'way',\n",
       " 'well',\n",
       " 'people',\n",
       " 'mr',\n",
       " 'little',\n",
       " 'state',\n",
       " 'good',\n",
       " 'make',\n",
       " 'world',\n",
       " 'still',\n",
       " 'see',\n",
       " 'men',\n",
       " 'work',\n",
       " 'long',\n",
       " 'get',\n",
       " 'life',\n",
       " 'never',\n",
       " 'day',\n",
       " 'another',\n",
       " 'know']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_wordlist[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# make a dictionary that maps each word to its index in the wordlist\n",
    "brown_wordlist_lookup = dict((word, index) for index, word in enumerate(brown_wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an array with enough space to hold \n",
    "# len(brown_wordlist) target words, and\n",
    "# len(brown_wordlist) context words.\n",
    "# We first initialize it to all zeros.\n",
    "brown_count_matrix = np.zeros((len(brown_wordlist), len(brown_wordlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's do the context word counting with this matrix.\n",
    "\n",
    "for sentence in brown_sents:\n",
    "    \n",
    "    for targetindex, target in enumerate(sentence):\n",
    "        for contextword in each_contextword_1wordwindow(sentence, targetindex):\n",
    "            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n",
    "                # which cell in the matrix is this? \n",
    "                # look up both the target and the context word\n",
    "                # in the ordered list of Brown words\n",
    "                targetindex_matrix = brown_wordlist_lookup[target]\n",
    "                contextindex_matrix = brown_wordlist_lookup[contextword]\n",
    "                # and add a count of one for this cell in the matrix\n",
    "                brown_count_matrix[targetindex_matrix][contextindex_matrix] += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    return 1 - scipy.spatial.distance.cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2667942904431144"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can again compute similarity in this space\n",
    "said_index = brown_wordlist_lookup[\"said\"]\n",
    "wrote_index = brown_wordlist_lookup[\"wrote\"]\n",
    "cosine_sim( brown_count_matrix[said_index], brown_count_matrix[wrote_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(metric=<function cosine at 0x7fec5d585e50>, n_neighbors=20)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nearest_neighbors_obj = NearestNeighbors(n_neighbors=20, metric = scipy.spatial.distance.cosine)\n",
    "\n",
    "# we then allow it to compute an internal datastructure from our data\n",
    "nearest_neighbors_obj.fit(brown_count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([brown_count_matrix[said_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "saidNN = pd.DataFrame(\n",
    "    np.c_[\n",
    "        np.array([brown_wordlist[i] for i in [target_indices][0][0]]), \n",
    "        1-cosine_distances[0]\n",
    "    ],\n",
    "    columns = ['word', 'cosine similarity']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cosine similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>say</td>\n",
       "      <td>0.5927040316696859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>knew</td>\n",
       "      <td>0.5598563510032091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>told</td>\n",
       "      <td>0.5548318969148726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>know</td>\n",
       "      <td>0.5505889042192217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>never</td>\n",
       "      <td>0.5208856639545308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>think</td>\n",
       "      <td>0.5117878164407484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>0.5111051872971337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>probably</td>\n",
       "      <td>0.5080811909281473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>make</td>\n",
       "      <td>0.506091311427728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>really</td>\n",
       "      <td>0.4932262266425065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>maybe</td>\n",
       "      <td>0.4858029651433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>seem</td>\n",
       "      <td>0.48217205453708345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>thought</td>\n",
       "      <td>0.48184845270369936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>give</td>\n",
       "      <td>0.47399531694442976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>come</td>\n",
       "      <td>0.46890345259503396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>happen</td>\n",
       "      <td>0.46610206757234174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>expect</td>\n",
       "      <td>0.4607338369844265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>done</td>\n",
       "      <td>0.4572185017879007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>remember</td>\n",
       "      <td>0.45150917240215893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word    cosine similarity\n",
       "0       said                  1.0\n",
       "1        say   0.5927040316696859\n",
       "2       knew   0.5598563510032091\n",
       "3       told   0.5548318969148726\n",
       "4       know   0.5505889042192217\n",
       "5      never   0.5208856639545308\n",
       "6      think   0.5117878164407484\n",
       "7       like   0.5111051872971337\n",
       "8   probably   0.5080811909281473\n",
       "9       make    0.506091311427728\n",
       "10    really   0.4932262266425065\n",
       "11     maybe   0.4858029651433333\n",
       "12      seem  0.48217205453708345\n",
       "13   thought  0.48184845270369936\n",
       "14      give  0.47399531694442976\n",
       "15      come  0.46890345259503396\n",
       "16    happen  0.46610206757234174\n",
       "17    expect   0.4607338369844265\n",
       "18      done   0.4572185017879007\n",
       "19  remember  0.45150917240215893"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saidNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_nearest_neighbors(word, n_neighbors=40, mtx=brown_count_matrix):\n",
    "    word_index = brown_wordlist_lookup[word]\n",
    "    nearest_neighbors_obj = NearestNeighbors(n_neighbors=n_neighbors, metric = scipy.spatial.distance.cosine)\n",
    "    nearest_neighbors_obj.fit(mtx)\n",
    "    cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([mtx[word_index]])\n",
    "    wordNN = pd.DataFrame(\n",
    "        np.c_[\n",
    "            np.array([brown_wordlist[i] for i in [target_indices][0][0]]), \n",
    "            1-cosine_distances[0]\n",
    "        ],\n",
    "        columns = ['word', 'cosine similarity']\n",
    "    )\n",
    "    return wordNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cosine similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bar</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bars</td>\n",
       "      <td>0.5564674946573845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>came</td>\n",
       "      <td>0.2475742495165294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yard</td>\n",
       "      <td>0.24304676496058508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seat</td>\n",
       "      <td>0.22488792440942396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>come</td>\n",
       "      <td>0.22066207993908915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>drew</td>\n",
       "      <td>0.2087698441872593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cap</td>\n",
       "      <td>0.20669184547858133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>af</td>\n",
       "      <td>0.2043401300234835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>forth</td>\n",
       "      <td>0.19680231539286097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>neck</td>\n",
       "      <td>0.19320676901410583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>went</td>\n",
       "      <td>0.19013262667575237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>barn</td>\n",
       "      <td>0.1819703004347767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>equation</td>\n",
       "      <td>0.1793324614449363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>curve</td>\n",
       "      <td>0.17650492332014212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>shoulder</td>\n",
       "      <td>0.1714157408456083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>goes</td>\n",
       "      <td>0.1708027882229436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>bond</td>\n",
       "      <td>0.1698845475423556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bring</td>\n",
       "      <td>0.16679412754901757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>line</td>\n",
       "      <td>0.1665092098833233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rear</td>\n",
       "      <td>0.1657221579473298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>c</td>\n",
       "      <td>0.16252875348605367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>moved</td>\n",
       "      <td>0.1624262788525449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>turned</td>\n",
       "      <td>0.15970074265453604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dates</td>\n",
       "      <td>0.15313451117716625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>coming</td>\n",
       "      <td>0.1530809577913096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pieces</td>\n",
       "      <td>0.14958102236374737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i.e</td>\n",
       "      <td>0.1494943681748513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>porch</td>\n",
       "      <td>0.14940105456617514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>knife</td>\n",
       "      <td>0.14927744395675324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>go</td>\n",
       "      <td>0.14758933222402937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bringing</td>\n",
       "      <td>0.14709431769615944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>points</td>\n",
       "      <td>0.1453557749010257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>brought</td>\n",
       "      <td>0.14516484328543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>stand</td>\n",
       "      <td>0.14400903252978936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>distance</td>\n",
       "      <td>0.14227420771218835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>far</td>\n",
       "      <td>0.14090669465133487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>getting</td>\n",
       "      <td>0.14028253003665625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>road</td>\n",
       "      <td>0.1388800518249832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>address</td>\n",
       "      <td>0.13766019544524455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word    cosine similarity\n",
       "0        bar                  1.0\n",
       "1       bars   0.5564674946573845\n",
       "2       came   0.2475742495165294\n",
       "3       yard  0.24304676496058508\n",
       "4       seat  0.22488792440942396\n",
       "5       come  0.22066207993908915\n",
       "6       drew   0.2087698441872593\n",
       "7        cap  0.20669184547858133\n",
       "8         af   0.2043401300234835\n",
       "9      forth  0.19680231539286097\n",
       "10      neck  0.19320676901410583\n",
       "11      went  0.19013262667575237\n",
       "12      barn   0.1819703004347767\n",
       "13  equation   0.1793324614449363\n",
       "14     curve  0.17650492332014212\n",
       "15  shoulder   0.1714157408456083\n",
       "16      goes   0.1708027882229436\n",
       "17      bond   0.1698845475423556\n",
       "18     bring  0.16679412754901757\n",
       "19      line   0.1665092098833233\n",
       "20      rear   0.1657221579473298\n",
       "21         c  0.16252875348605367\n",
       "22     moved   0.1624262788525449\n",
       "23    turned  0.15970074265453604\n",
       "24     dates  0.15313451117716625\n",
       "25    coming   0.1530809577913096\n",
       "26    pieces  0.14958102236374737\n",
       "27       i.e   0.1494943681748513\n",
       "28     porch  0.14940105456617514\n",
       "29     knife  0.14927744395675324\n",
       "30        go  0.14758933222402937\n",
       "31  bringing  0.14709431769615944\n",
       "32    points   0.1453557749010257\n",
       "33   brought     0.14516484328543\n",
       "34     stand  0.14400903252978936\n",
       "35  distance  0.14227420771218835\n",
       "36       far  0.14090669465133487\n",
       "37   getting  0.14028253003665625\n",
       "38      road   0.1388800518249832\n",
       "39   address  0.13766019544524455"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nearest_neighbors(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def each_contextword_2wordwindow(wordlist, targetindex):\n",
    "    if targetindex > 0:\n",
    "        # preceding words\n",
    "        yield wordlist[targetindex - 1]\n",
    "    if targetindex > 1:\n",
    "        yield wordlist[targetindex - 2]\n",
    "        \n",
    "    if targetindex < len(wordlist)- 1:\n",
    "        # succeeding words\n",
    "        yield wordlist[targetindex + 1]\n",
    "    if targetindex < len(wordlist)- 2:\n",
    "        # succeeding words\n",
    "        yield wordlist[targetindex + 2]        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_context_counts2 = nltk.ConditionalFreqDist()\n",
    "\n",
    "frequency_threshold = 20\n",
    "\n",
    "for sentence in brown_sents:\n",
    "    \n",
    "    for targetindex, target in enumerate(sentence):\n",
    "        for contextword in each_contextword_2wordwindow(sentence, targetindex):\n",
    "            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n",
    "                brown_context_counts2[target][contextword] += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent contexts for some targets:\n",
      "\n",
      "election:\n",
      " [('day', 5), ('presidential', 5), ('results', 4), ('back', 3), ('campaign', 3), ('judges', 3), ('november', 3), ('board', 3), ('primary', 2), ('recent', 2)]\n",
      "love:\n",
      " [('man', 9), ('god', 8), ('us', 7), ('way', 6), ('know', 5), ('like', 5), ('even', 5), ('never', 5), ('give', 4), ('said', 4)]\n",
      "car: [('got', 9), ('one', 9), ('back', 9), ('said', 8), ('police', 8), ('driving', 8), ('around', 7), ('road', 7), ('parked', 7), ('take', 6)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"10 most frequent contexts for some targets:\\n\")\n",
    "print(\"election:\\n\", brown_context_counts2[\"election\"].most_common(10))\n",
    "print(\"love:\\n\", brown_context_counts2[\"love\"].most_common(10))\n",
    "print(\"car:\", brown_context_counts2[\"car\"].most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some ambiguous words:\n",
      "bank:\n",
      " [('washington', 7), ('national', 3), ('prepared', 3), ('west', 3), ('red', 2), ('french', 2), ('new', 2), ('local', 2), ('south', 2), ('held', 2), ('east', 2), ('manchester', 2), ('edge', 2), ('river', 2), ('take', 1), ('accounts', 1), ('confidence', 1), ('customers', 1), ('also', 1), ('even', 1), ('savings', 1), ('chicago', 1), ('first', 1), ('jersey', 1), ('installed', 1), ('visited', 1), ('march', 1), ('people', 1), ('said', 1), ('touch', 1), ('reserve', 1), ('done', 1), ('policy', 1), ('make', 1), ('cloud', 1), ('hope', 1), ('representative', 1), ('end', 1), ('would', 1), ('world', 1), ('firm', 1), ('later', 1), ('handling', 1), ('sighed', 1), ('president', 1), ('destroy', 1), ('southern', 1), ('approval', 1), ('contact', 1), ('commerce', 1), ('officials', 1), ('cash', 1), ('may', 1), ('loans', 1), ('whether', 1), ('received', 1), ('manhattan', 1), ('providence', 1), ('left', 1), ('north', 1), ('put', 1), ('green', 1), ('man', 1), ('brought', 1), ('coming', 1), ('hudson', 1), ('waiting', 1), ('wrong', 1), ('far', 1), ('soft', 1), ('back', 1), ('toward', 1), ('stumbled', 1), ('passed', 1), ('mrs', 1), ('appointed', 1), ('cap', 1), ('go', 1), ('high', 1), ('outside', 1), ('big', 1), ('roll', 1)]\n",
      "bar:\n",
      " [('locking', 10), ('back', 4), ('end', 4), ('association', 3), ('af', 3), ('running', 2), ('rest', 2), ('top', 2), ('patent', 2), ('midnight', 2), ('atlanta', 1), ('would', 1), ('vehicles', 1), ('chicago', 1), ('american', 1), ('body', 1), ('without', 1), ('singing', 1), ('cocktail', 1), ('feel', 1), ('come', 1), ('hold', 1), ('experiments', 1), ('held', 1), ('12', 1), ('lower', 1), ('stock', 1), ('across', 1), ('remain', 1), ('c', 1), ('proper', 1), ('lines', 1), ('rough', 1), ('sink', 1), ('recommended', 1), ('eating', 1), ('effort', 1), ('introduced', 1), ('mr', 1), ('long', 1), ('admitted', 1), ('career', 1), ('considerable', 1), ('judge', 1), ('day', 1), ('legend', 1), ('temple', 1), ('absolute', 1), ('recovery', 1), ('however', 1), ('finding', 1), ('little', 1), ('next', 1), ('hanging', 1), ('oil', 1), ('breakfast', 1), ('kitchen', 1), ('main', 1), ('beside', 1), ('bill', 1), ('front', 1), ('headed', 1), ('called', 1), ('two', 1), ('b', 1), ('forth', 1), (\"he's\", 1), ('view', 1), ('windows', 1), ('met', 1), ('attempt', 1), ('taking', 1), ('towards', 1), ('probably', 1), ('crowded', 1), ('remember', 1), ('san', 1)]\n",
      "leave:\n",
      " [('us', 5), ('alone', 5), ('france', 5), ('would', 5), ('want', 5), ('take', 4), (\"can't\", 4), ('room', 3), ('morning', 3), ('one', 3), ('country', 3), ('better', 3), ('decided', 3), ('instructions', 3), ('could', 3), (\"i'll\", 3), ('party', 3), ('get', 2), ('behind', 2), ('right', 2), ('going', 2), ('let', 2), ('parker', 2), ('peace', 2), ('clay', 2), ('table', 2), ('people', 2), ('cannot', 2), ('farm', 2), ('never', 2), ('permission', 2), ('alfred', 2), ('extend', 2), ('nest', 2), ('job', 2), (\"he'd\", 2), ('wife', 2), ('time', 2), ('last', 2), ('sure', 2), ('eisenhower', 1), ('capitol', 1), ('coach', 1), ('sunday', 1), ('set', 1), ('church', 1), ('aid', 1), ('confused', 1), ('minute', 1), ('home', 1), ('applications', 1), ('compromise', 1), ('sides', 1), ('heard', 1), ('upon', 1), ('feeling', 1), ('association', 1), ('took', 1), ('learned', 1), ('power', 1), ('world', 1), ('open', 1), ('c', 1), ('mold', 1), ('seldom', 1), ('storm', 1), ('excuse', 1), ('work', 1), ('paper', 1), ('interest', 1), ('company', 1), ('car', 1), ('europe', 1), ('arrival', 1), ('another', 1), ('permit', 1), ('rent', 1), ('service', 1), ('first', 1), ('completely', 1), ('trace', 1), ('asked', 1), ('hotel', 1), ('forced', 1), ('cattle', 1), ('caused', 1), ('returned', 1), ('received', 1), ('effect', 1), ('pure', 1), ('simply', 1), ('negroes', 1), ('request', 1), ('earlier', 1), ('saw', 1), ('side', 1), ('opposite', 1), ('account', 1), ('ever', 1), ('house', 1)]\n"
     ]
    }
   ],
   "source": [
    "# some ambiguous words\n",
    "print(\"Some ambiguous words:\")\n",
    "print(\"bank:\\n\", brown_context_counts[\"bank\"].most_common(100))\n",
    "print(\"bar:\\n\", brown_context_counts[\"bar\"].most_common(100))\n",
    "print(\"leave:\\n\", brown_context_counts[\"leave\"].most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We need an array with enough space to hold \n",
    "# len(brown_wordlist) target words, and\n",
    "# len(brown_wordlist) context words.\n",
    "# We first initialize it to all zeros.\n",
    "brown_count_matrix2 = np.zeros((len(brown_wordlist), len(brown_wordlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's do the context word counting with this matrix.\n",
    "\n",
    "for sentence in brown_sents:\n",
    "    \n",
    "    for targetindex, target in enumerate(sentence):\n",
    "        for contextword in each_contextword_2wordwindow(sentence, targetindex):\n",
    "            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n",
    "                # which cell in the matrix is this? \n",
    "                # look up both the target and the context word\n",
    "                # in the ordered list of Brown words\n",
    "                targetindex_matrix = brown_wordlist_lookup[target]\n",
    "                contextindex_matrix = brown_wordlist_lookup[contextword]\n",
    "                # and add a count of one for this cell in the matrix\n",
    "                brown_count_matrix2[targetindex_matrix][contextindex_matrix] += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37804462153956675"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can again compute similarity in this space\n",
    "said_index = brown_wordlist_lookup[\"said\"]\n",
    "wrote_index = brown_wordlist_lookup[\"wrote\"]\n",
    "cosine_sim( brown_count_matrix2[said_index], brown_count_matrix2[wrote_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cosine similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>british</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>american</td>\n",
       "      <td>0.2524333133651866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>0.2511645493907131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people</td>\n",
       "      <td>0.24812185850139756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>industry</td>\n",
       "      <td>0.24471402611744353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>german</td>\n",
       "      <td>0.24206729609336974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>english</td>\n",
       "      <td>0.24118829043684986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>market</td>\n",
       "      <td>0.24022179105214925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>called</td>\n",
       "      <td>0.23774731774246216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>history</td>\n",
       "      <td>0.23570425624010394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>part</td>\n",
       "      <td>0.23319152867933268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>culture</td>\n",
       "      <td>0.22908480468697268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>society</td>\n",
       "      <td>0.2280101672374203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>revolution</td>\n",
       "      <td>0.22146232898275586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>would</td>\n",
       "      <td>0.22054507338232565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>army</td>\n",
       "      <td>0.21787418532923408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>india</td>\n",
       "      <td>0.21462575233231818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>one</td>\n",
       "      <td>0.21435089673130903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>man</td>\n",
       "      <td>0.21286524462753764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>indians</td>\n",
       "      <td>0.21247922286054255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>way</td>\n",
       "      <td>0.211207666697398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>folklore</td>\n",
       "      <td>0.21011515974645167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>french</td>\n",
       "      <td>0.20713819363506003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>made</td>\n",
       "      <td>0.20704236152180444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>well</td>\n",
       "      <td>0.20502893327567806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>london</td>\n",
       "      <td>0.20491147610942784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>league</td>\n",
       "      <td>0.20435203090451481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>work</td>\n",
       "      <td>0.20373676158655118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>might</td>\n",
       "      <td>0.2029517105697798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>life</td>\n",
       "      <td>0.20283726902290555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>fact</td>\n",
       "      <td>0.2022746402588801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>formed</td>\n",
       "      <td>0.20183989606593622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>said</td>\n",
       "      <td>0.20149936691003978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>public</td>\n",
       "      <td>0.20096030423367428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>today</td>\n",
       "      <td>0.2006188156667318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>indian</td>\n",
       "      <td>0.20055589912966243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>must</td>\n",
       "      <td>0.20048410710438636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>trade</td>\n",
       "      <td>0.19911160113590853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>association</td>\n",
       "      <td>0.19894824119042231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>good</td>\n",
       "      <td>0.19741131349928254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word    cosine similarity\n",
       "0       british                  1.0\n",
       "1      american   0.2524333133651866\n",
       "2      politics   0.2511645493907131\n",
       "3        people  0.24812185850139756\n",
       "4      industry  0.24471402611744353\n",
       "5        german  0.24206729609336974\n",
       "6       english  0.24118829043684986\n",
       "7        market  0.24022179105214925\n",
       "8        called  0.23774731774246216\n",
       "9       history  0.23570425624010394\n",
       "10         part  0.23319152867933268\n",
       "11      culture  0.22908480468697268\n",
       "12      society   0.2280101672374203\n",
       "13   revolution  0.22146232898275586\n",
       "14        would  0.22054507338232565\n",
       "15         army  0.21787418532923408\n",
       "16        india  0.21462575233231818\n",
       "17          one  0.21435089673130903\n",
       "18          man  0.21286524462753764\n",
       "19      indians  0.21247922286054255\n",
       "20          way    0.211207666697398\n",
       "21     folklore  0.21011515974645167\n",
       "22       french  0.20713819363506003\n",
       "23         made  0.20704236152180444\n",
       "24         well  0.20502893327567806\n",
       "25       london  0.20491147610942784\n",
       "26       league  0.20435203090451481\n",
       "27         work  0.20373676158655118\n",
       "28        might   0.2029517105697798\n",
       "29         life  0.20283726902290555\n",
       "30         fact   0.2022746402588801\n",
       "31       formed  0.20183989606593622\n",
       "32         said  0.20149936691003978\n",
       "33       public  0.20096030423367428\n",
       "34        today   0.2006188156667318\n",
       "35       indian  0.20055589912966243\n",
       "36         must  0.20048410710438636\n",
       "37        trade  0.19911160113590853\n",
       "38  association  0.19894824119042231\n",
       "39         good  0.19741131349928254"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nearest_neighbors(\"british\", 40, brown_count_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The curse of dimensionality\n",
    "\n",
    "Our data matrix has 23,464,336 cells, which have to be estimated from 537,755 word tokens. From a parameter estimation perspective, this is outright silly.\n",
    "\n",
    "Apart from that, high-dimensional feature spaces bring some problems known as the **curse of dimensionality**, such as\n",
    "\n",
    "- computations are inefficient\n",
    "- risk of overtraining (rule of thumb: 5 training examples per dimension are the minimum)\n",
    "\n",
    "Common practice: project points from high-dimensional surface feature space to lower-dimensional **latent feature space**.\n",
    "\n",
    "Today we will look at **Singular Value Decomposition** (SVD), a fairly simple dimensionality reduction technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrices as images\n",
    "\n",
    "We can visualize a (small) matrix by converting values to colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.array([[-1, 2, 3],\n",
    "                 [4, -5, 6],\n",
    "                 [7, 8, -9]])\n",
    "\n",
    "plt.imshow(data, cmap='seismic', vmin=-9, vmax=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the following matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 & 1 & 1 &\\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 &\\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 &\\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 &\\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 &\\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 &\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.ones((6,6)), cmap=\"seismic\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This matrix can be compressed as the product of two vectors:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\1\\\\1\\\\1\\\\1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1&1&1&1&1&1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is a **rank-1** matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The French flag\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & 0 & 0 & 1 & 1 &\\\\\n",
    "-1 & -1 & 0 & 0 & 1 & 1 &\\\\\n",
    "-1 & -1 & 0 & 0 & 1 & 1 &\\\\\n",
    "-1 & -1 & 0 & 0 & 1 & 1 &\\\\\n",
    "-1 & -1 & 0 & 0 & 1 & 1 &\\\\\n",
    "-1 & -1 & 0 & 0 & 1 & 1 &\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fec2f5e6ac0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow([\n",
    "    [-1,-1,0,0,1,1],\n",
    "    [-1,-1,0,0,1,1],\n",
    "    [-1,-1,0,0,1,1],\n",
    "    [-1,-1,0,0,1,1],\n",
    "    [-1,-1,0,0,1,1],\n",
    "    [-1,-1,0,0,1,1]\n",
    "], cmap=\"seismic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is still a rank-1 matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1\\\\1\\\\1\\\\1\\\\1\\\\1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-1&-1&0&0&1&1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some more rank-1 matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03, 0.01, 0.04, 0.04, 0.07, 0.04, 0.07, 0.03, 0.05, 0.04, 0.03,\n",
       "        0.06, 0.02, 0.03, 0.07, 0.08, 0.09, 0.07, 0.06, 0.06],\n",
       "       [0.27, 0.06, 0.38, 0.42, 0.78, 0.41, 0.8 , 0.33, 0.57, 0.41, 0.35,\n",
       "        0.69, 0.17, 0.32, 0.79, 0.8 , 0.94, 0.73, 0.65, 0.7 ],\n",
       "       [0.23, 0.05, 0.32, 0.36, 0.66, 0.35, 0.68, 0.28, 0.48, 0.35, 0.3 ,\n",
       "        0.59, 0.15, 0.27, 0.67, 0.69, 0.8 , 0.62, 0.56, 0.59],\n",
       "       [0.14, 0.03, 0.19, 0.21, 0.39, 0.21, 0.4 , 0.16, 0.28, 0.2 , 0.17,\n",
       "        0.34, 0.09, 0.16, 0.39, 0.4 , 0.47, 0.36, 0.32, 0.35],\n",
       "       [0.14, 0.03, 0.2 , 0.22, 0.4 , 0.22, 0.42, 0.17, 0.29, 0.21, 0.18,\n",
       "        0.36, 0.09, 0.17, 0.41, 0.42, 0.49, 0.38, 0.34, 0.36],\n",
       "       [0.07, 0.02, 0.1 , 0.11, 0.19, 0.1 , 0.2 , 0.08, 0.14, 0.1 , 0.09,\n",
       "        0.17, 0.04, 0.08, 0.2 , 0.2 , 0.24, 0.18, 0.16, 0.17],\n",
       "       [0.04, 0.01, 0.06, 0.07, 0.12, 0.07, 0.13, 0.05, 0.09, 0.06, 0.05,\n",
       "        0.11, 0.03, 0.05, 0.12, 0.13, 0.15, 0.11, 0.1 , 0.11],\n",
       "       [0.19, 0.04, 0.27, 0.3 , 0.55, 0.29, 0.56, 0.23, 0.4 , 0.29, 0.24,\n",
       "        0.49, 0.12, 0.23, 0.55, 0.57, 0.66, 0.51, 0.46, 0.49],\n",
       "       [0.14, 0.03, 0.19, 0.21, 0.38, 0.2 , 0.4 , 0.16, 0.28, 0.2 , 0.17,\n",
       "        0.34, 0.09, 0.16, 0.39, 0.4 , 0.47, 0.36, 0.32, 0.34],\n",
       "       [0.04, 0.01, 0.05, 0.06, 0.11, 0.06, 0.12, 0.05, 0.08, 0.06, 0.05,\n",
       "        0.1 , 0.03, 0.05, 0.11, 0.12, 0.14, 0.11, 0.09, 0.1 ],\n",
       "       [0.24, 0.05, 0.33, 0.36, 0.67, 0.36, 0.69, 0.28, 0.49, 0.35, 0.3 ,\n",
       "        0.6 , 0.15, 0.28, 0.68, 0.69, 0.81, 0.63, 0.56, 0.6 ],\n",
       "       [0.21, 0.05, 0.29, 0.33, 0.6 , 0.32, 0.62, 0.25, 0.44, 0.32, 0.27,\n",
       "        0.54, 0.13, 0.25, 0.61, 0.62, 0.72, 0.56, 0.5 , 0.54],\n",
       "       [0.24, 0.06, 0.33, 0.37, 0.67, 0.36, 0.69, 0.28, 0.49, 0.35, 0.3 ,\n",
       "        0.6 , 0.15, 0.28, 0.68, 0.7 , 0.81, 0.63, 0.56, 0.6 ],\n",
       "       [0.09, 0.02, 0.13, 0.15, 0.27, 0.14, 0.28, 0.11, 0.2 , 0.14, 0.12,\n",
       "        0.24, 0.06, 0.11, 0.27, 0.28, 0.32, 0.25, 0.23, 0.24],\n",
       "       [0.19, 0.04, 0.26, 0.29, 0.53, 0.28, 0.55, 0.22, 0.39, 0.28, 0.24,\n",
       "        0.47, 0.12, 0.22, 0.54, 0.55, 0.64, 0.5 , 0.44, 0.47],\n",
       "       [0.08, 0.02, 0.11, 0.12, 0.23, 0.12, 0.23, 0.1 , 0.17, 0.12, 0.1 ,\n",
       "        0.2 , 0.05, 0.09, 0.23, 0.24, 0.28, 0.21, 0.19, 0.2 ],\n",
       "       [0.19, 0.04, 0.26, 0.29, 0.54, 0.29, 0.56, 0.23, 0.39, 0.28, 0.24,\n",
       "        0.48, 0.12, 0.22, 0.55, 0.56, 0.65, 0.51, 0.45, 0.48],\n",
       "       [0.19, 0.04, 0.27, 0.3 , 0.54, 0.29, 0.56, 0.23, 0.4 , 0.29, 0.24,\n",
       "        0.49, 0.12, 0.23, 0.55, 0.56, 0.66, 0.51, 0.46, 0.49],\n",
       "       [0.23, 0.05, 0.31, 0.35, 0.64, 0.34, 0.66, 0.27, 0.47, 0.34, 0.29,\n",
       "        0.57, 0.14, 0.27, 0.65, 0.67, 0.78, 0.6 , 0.54, 0.57],\n",
       "       [0.27, 0.06, 0.38, 0.42, 0.77, 0.41, 0.79, 0.33, 0.56, 0.41, 0.34,\n",
       "        0.69, 0.17, 0.32, 0.78, 0.8 , 0.93, 0.72, 0.65, 0.69]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 20\n",
    "\n",
    "A = np.random.uniform(size=n).reshape((n,1)) @ np.random.uniform(size=n).reshape((1,n))\n",
    "np.around(A,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fec2f571550>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(A, cmap=\"seismic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Higher-rank matrices\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "1 & 1\n",
    "\\end{bmatrix} = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\begin{bmatrix}1&1\\end{bmatrix}-\n",
    "\\begin{bmatrix}1\\\\0\\end{bmatrix}\\begin{bmatrix}0&1\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can trivially decompose every $m\\times n$ matrix into $m$ rank-1 matrices:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}\n",
    "A_{1,\\_}\\\\\n",
    "\\vdots\\\\\n",
    "A_{m,\\_}\n",
    "\\end{bmatrix} &= \n",
    "\\begin{bmatrix}\n",
    "1\\\\0\\\\\\vdots \\\\ 0 \n",
    "\\end{bmatrix} A_{1,\\_} + \n",
    "\\begin{bmatrix}\n",
    "0\\\\1\\\\\\vdots \\\\ 0 \n",
    "\\end{bmatrix} A_{2,\\_} + \\cdots\n",
    "\\begin{bmatrix}\n",
    "0\\\\0\\\\\\vdots \\\\ 1\n",
    "\\end{bmatrix} A_{m,\\_}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A less trivial task: Given a matrix $A$, find the rank-1 matrix $B$ which minimizes the squared error\n",
    "$$\n",
    "\\sum_{i,j} (a_{i,j}-b_{i,j})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fast forward: some advanced linear algebra\n",
    "\n",
    "- A matrix $S$ is called **symmetric** if $S = S^T$.\n",
    "- A symmetric matrix $S$ is called **positive semidefinite** if for all vectors $\\mathbf x$:\n",
    "$$\n",
    "\\|S\\mathbf x\\| \\geq 0\n",
    "$$\n",
    "\n",
    "### Theorem\n",
    "\n",
    "For each matrix $A$, $A^TA$ and $AA^T$ are positive semidefinite.\n",
    "\n",
    "### Theorem (Spectral Theorem)\n",
    "\n",
    "For each symmetric $n\\times n$ matricx $S$, there is an $n\\times n$ matrix $U$ and an $n\\times n$ diagonal matrix $\\Lambda$ such that\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    U^{-1} &= U^T\\\\\n",
    "    S &= U\\Lambda U^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If $S$ is positive semi-definite, all entries in $\\Lambda$ are $\\geq 0$.\n",
    "\n",
    "We assume without loss of generality that the diagonal entries of $\\Lambda$ are ordered in descending size.\n",
    "\n",
    "The columns of $U$ are called **eigenvectors** and the diagonal entries of $\\Lambda$ **eigenvalues**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deriving the SVD\n",
    "\n",
    "- Let $A$ be a non-zero $n\\times m$ matrix, with $n\\leq m$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $AA^T$ is symmetric and positive semidefinite. Therefore there are $n\\times n$ matries $U, \\Lambda$ with\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "U^{-1} &= U^T\\\\\n",
    "\\Lambda &\\mbox{ is a non-negative diagonal matrix, with entries in descending order}\\\\\n",
    "AA^T &= U\\Lambda U^T\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $\\mathbf u_i$ be the $i$th column of $U$, $\\lambda_i (=\\Lambda_{i,i})\\neq 0$, and let $\\mathbf x_i = A^T\\mathbf u_i$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|\\mathbf x_i\\| &= \\|A^T \\mathbf u_i\\|\\\\\n",
    "&= \\mathbf u_i^T AA^T \\mathbf u_i\\\\\n",
    "&= \\mathbf u_i^T \\lambda_i \\mathbf u_i\\\\\n",
    "&= \\lambda_i \\mathbf u_i^T \\mathbf u_i\\\\\n",
    "&= \\lambda_i \\|\\mathbf u_i^T\\|^2\\\\\n",
    "\\|\\mathbf x_i\\| &= \\sqrt{\\lambda_i}\\|\\mathbf u_i^T\\|\\\\\n",
    "\\sigma_i &\\doteq \\sqrt{\\lambda_i}\\\\\n",
    "\\mathbf v_i &\\doteq \\frac{1}{\\sigma_i} A^T\\mathbf u_i\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore all $\\mathbf v_i$ are unit-vectors, i.e., $\\|\\mathbf v_i\\|=1$.\n",
    "\n",
    "Now let $i\\neq j$ and $\\lambda_i, \\lambda_j\\neq 0$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf v_i^T \\mathbf v_j &= (\\frac{1}{\\sigma_i}A^T\\mathbf u_i)^T(\\frac{1}{\\sigma_j}A^T\\mathbf u_j)\\\\\n",
    "&= \\frac{1}{\\sigma_i\\sigma_j}\\mathbf u_i^T AA^T \\mathbf u_j\\\\\n",
    "&= \\frac{1}{\\sigma_i\\sigma_j}\\mathbf u_i^T \\lambda_j \\mathbf u_j\\\\\n",
    "&= \\frac{\\sigma_j}{\\sigma_i}\\mathbf u_i^T \\mathbf u_j\\\\\n",
    "&= \\mathbf 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So if $i\\neq j$ and $\\lambda_i, \\lambda_j\\neq 0$, $\\mathbf u_i$ and $\\mathbf u_j$ are perpendicular to each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally:\n",
    "    \n",
    "Let $\\sigma_i\\neq 0$. \n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^TA\\mathbf v_i &= A^T A (\\frac{1}{\\sigma_i}A^T\\mathbf u_i)\\\\\n",
    "&= \\frac{1}{\\sigma_i}A^T A A^T\\mathbf u_i\\\\\n",
    "&= \\frac{1}{\\sigma_i}A^T (\\lambda_i\\mathbf u_i)\\\\\n",
    "&= \\sigma_i A^T \\mathbf u_i\\\\\n",
    "&= \\lambda_i \\mathbf v_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$AA^T$ and $A^TA$ have the same non-zero eigenvectors, and all $u_i$ are eigenvectors of $A^TA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### wrapping everything up\n",
    "\n",
    "We have\n",
    "\n",
    "$$A^T \\mathbf u_i = \\sigma_i \\mathbf v_i$$\n",
    "\n",
    "for all $i$ with $\\sigma_i\\neq 0$. $\\mathbf u_i$ is an eigenvector of $AA^T$ and $\\mathbf v_i$ an eigenvector of $A^TA$. \n",
    "\n",
    "If $\\sigma_i=0$, then $\\lambda_i=0$, i.e.\n",
    "\n",
    "$$\n",
    "AA^T \\mathbf u_i = \\mathbf 0\n",
    "$$\n",
    "\n",
    "We have\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\|A^T\\mathbf u_i\\|^2 &= \\mathbf u_i^TAA^T\\mathbf u_i\\\\\n",
    "&= 0\\\\\n",
    "A^T\\mathbf u_i &= \\mathbf 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, even if $\\sigma_i=0$:\n",
    "\n",
    "$$A^T \\mathbf u_i = \\sigma_i \\mathbf v_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can combine this for all $i$ to get\n",
    "\n",
    "$$\n",
    "A^T U = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf v_i \\cdots \\mathbf v_n\n",
    "\\end{bmatrix}\\Sigma_{1\\cdots n} \n",
    "$$\n",
    "\n",
    "Where $\\Sigma_{1\\cdots n}$ is an $n\\times n$ diagonal matrix with $\\sigma_i$ in the $i$th diagonal cell.\n",
    "\n",
    "If $m>n$ we can add all eigenvectors of $A^TA$ to $\\begin{bmatrix}\n",
    "\\mathbf v_i \\cdots \\mathbf v_n\n",
    "\\end{bmatrix}$\n",
    "which correspond to the eigenvector $0$. This completes the eigenvector matrix of $A^TA$.\n",
    "\n",
    "Likewise, we extend $\\Sigma_{1\\cdots n}$ to an $m\\times n$ matrix $\\Sigma^T$ by adding all-zero rows at the end.\n",
    "\n",
    "This yields:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^TU &= V\\Sigma^T \\\\\n",
    "A^T &= V\\Sigma^T U^T\\\\\n",
    "A &= U\\Sigma V^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "The last line is the **Singular Value Decomposition** of the matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The rank-1 matrix closest to $A$ is constructed by keeping $\\sigma_1$ and setting all other cells in $\\Sigma$ to 0.\n",
    "\n",
    "Likewise, the rank-2 matrix closest to $A$ is constructed by keeping $\\sigma_1$ and $\\sigma_2$ and setting all other cells in $\\Sigma$ to 0 etc.\n",
    "\n",
    "Here is a nice demo: http://timbaumann.info/svd-image-compression-demo/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python3_vss",
   "language": "python",
   "name": "python3_vss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
